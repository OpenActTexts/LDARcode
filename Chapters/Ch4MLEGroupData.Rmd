---
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    pandoc_args: [
      "--number-sections",
      "--number-offset=3"
    ]
---
# Model Selection 

*This file contains illustrative **R** code for computing important count distributions. When reviewing this code, you should open an **R** session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go.This code uses the dataset `CLAIMLEVEL.csv` *

## Claim Level Data of Property Fund {.tabset}
This section summarizes claims from the property fund for year 2010 and plots the data. 

### Claims Data
The results below considers individual claims from the property fund for year 2010.
```{r}
## Read in data and get number of claims.  
claim_lev <- read.csv("Data/CLAIMLEVEL.csv", header = TRUE) 
nrow(claim_lev)  # 6258

# 2010 subset 
claim_data <- subset(claim_lev, Year == 2010); 
length(unique(claim_data$PolicyNum))  # 403 unique policyholders
n_tot <- nrow(claim_data)  # 1377 individual claims
n_tot
# As an alternative, you can simulate claims
# n_tot <- 13770
# alpha_hat <- 2
# theta_hat <- 100
# claim <- rgamma(n_tot, shape = alpha_hat, scale = theta_hat)
# claim <- rparetoII(n_tot, loc = 0,  shape = alpha_hat, scale = theta_hat)
# GB2
# claim <- theta_hat * rgamma(n_tot, shape = alpha_hat, scale = 1) / 
#          rgamma(n_tot, shape = 1, scale = 1) 
# claim_data <- data.frame(claim)

###################################################
```

### Summary of Claims
The output below provides summary on claims data for 2010 and summary in logarithmic units. 
```{r}
# Summarizing the claim data for 2010
summary(claim_data$Claim)
sd(claim_data$Claim)

# Summarizing logarithmic claims for 2010
summary(log(claim_data$Claim))
sd(log(claim_data$Claim))
```


### Plot of Claims
The plots below provides further information about the distribution of sample claims. 

```{r}
# Histogram 
par(mfrow = c(1, 2))
hist(claim_data$Claim, main="", xlab = "Claims")
hist(log(claim_data$Claim), main = "", xlab = "Logarithmic Claims")
# dev.off()
```

## Fitting Distributions {.tabset}
This section shows how to fit basic distributions to a data set. 

### Inference Assuming a Lognormal Distribution
The results below assume that the data follow a lognormal distribution and uses`VGAM` library for estimation of parameters. 
```{r warning = FALSE, message = FALSE}
# Inference assuming a lognormal distribution
# First, take the log of the data and assume normality
y <- log(claim_data$Claim)
summary(y)
sd(y)
# Confidence intervals and hypothesis test
t.test(y, mu = log(5000))  # H0: mu_o = log(5000) = 8.517

# Mean of the lognormal distribution
exp(mean(y) + sd(y)^2 / 2)
mean(claim_data$Claim)

# Alternatively, assume that the data follow a lognormal distribution
# Use "VGAM" library for estimation of parameters 
library(VGAM)
fit.LN <- vglm(Claim ~ 1, family = lognormal, data = claim_data)
summary(fit.LN)
coef(fit.LN)                   # coefficients
confint(fit.LN, level = 0.95)  # confidence intervals for model parameters 
logLik(fit.LN)                 # loglikelihood for lognormal
AIC(fit.LN)                    # AIC for lognormal
BIC(fit.LN)                    # BIC for lognormal
vcov(fit.LN)                   # covariance matrix for model parameters 

# Mean of the lognormal distribution
exp(mean(y) + sd(y)^2 / 2)

exp(coef(fit.LN))

```

A few quick notes on these commands:

* The `t.test()` function can be used for a variety of t-tests. In this illustration, it was used to test $H_0=\mu_0=\log(5000)=8.517$.
* The `vglm()` function is used to fit vector generalized linear models (VGLMs). See `help(vglm)` for other modeling options.
* The `coef()` function returns the estimated coefficients from the `vglm` or other modeling functions. 
* The `confint` function provides the confidence intervals for model parameters. 
* The `loglik` function provides the log-likelihood value for the lognormal estimation from the `vglm` or other modeling functions.
* `AIC()` and `BIC()` returns Akaike's Information Criterion and BIC or SBC (Schwarz's Bayesian criterion) for the fitted lognormal model. $\text{AIC} =-2* \text{(loglikelihood)} + 2*\text{npar}$ , where `npar` represents the number of parameters in the fitted model, and $\text{BIC} =-2* \text{log-likelihood} +  \log(n)* \text{npar}$ where $n$ is the number of observations. 
* `vcov()` returns the covariance matrix for model parameters. 

### Inference Assuming a Gamma Distribution
The results below assume that the data follow a gamma distribution and uses`VGAM` library for estimation of parameters. 

```{r}
# Inference assuming a gamma distribution
# Install.packages("VGAM")
library(VGAM)
fit.gamma <- vglm(Claim ~ 1, family = gamma2, data = claim_data)
summary(fit.gamma)
coef(fit.gamma)                 # This uses a different parameterization 

( theta <- exp(coef(fit.gamma)[1]) / exp(coef(fit.gamma)[2]) )  # theta = mu / alpha
( alpha <- exp(coef(fit.gamma)[2]) )

plot(density(log(claim_data$Claim)), main = "", xlab = "Log Expenditures")
x <- seq(0, 15, by = 0.01)
fgamma_ex <- dgamma(exp(x), shape = alpha, scale = theta) * exp(x)
lines(x, fgamma_ex, col = "blue")

confint(fit.gamma, level = 0.95)  # confidence intervals for model parameters 
logLik(fit.gamma)                 # loglikelihood for gamma
AIC(fit.gamma)                    # AIC for gamma
BIC(fit.gamma)                    # BIC for gamma
vcov(fit.gamma)                   # covariance matrix for model parameters 

# Here is a check on the formulas
# AIC using formula : -2 * (loglik) + 2 * (number of parameters)
-2 * (logLik(fit.gamma)) + 2 * (length(coef(fit.gamma)))
# BIC using formula : -2 * (loglik) + (number of parameters) * (log(n))
-2 * (logLik(fit.gamma)) + length(coef(fit.gamma, matrix = TRUE)) * log(nrow(claim_data))

# Alternatively, we could a gamma distribution using glm
library(MASS)
fit.gamma_2 <- glm(Claim ~ 1, data = claim_data, family = Gamma(link = log)) 
summary(fit.gamma_2, dispersion = gamma.dispersion(fit.gamma_2)) 

( theta <- exp(coef(fit.gamma_2)) * gamma.dispersion(fit.gamma_2) )  #theta = mu / alpha
( alpha <- 1 / gamma.dispersion(fit.gamma_2) )

logLik(fit.gamma_2)  # log - likelihood slightly different from vglm
AIC(fit.gamma_2)     # AIC
BIC(fit.gamma_2)     # BIC

```



Note : The output from `coef(fit.gamma)` uses the parameterization $\mu = \theta * \alpha$.  `coef(fit.gamma)[1]` = $\log(\mu)$ and  `coef(fit.gamma)[2]` = $\log(\alpha)$, which implies , $\alpha$ = `exp(coef(fit.gamma)[2])` and $\theta = \mu / \alpha$ = `exp(coef(fit.gamma)[1]) / exp(coef(fit.gamma)[2])`.




### Inference Assuming a Pareto Distribution
The results below assume that the data follow a Pareto distribution and uses`VGAM` library for estimation of parameters. 

```{r}
fit.pareto <- vglm(Claim ~ 1, paretoII, loc = 0, data = claim_data)
summary(fit.pareto)
head(fitted(fit.pareto))
coef(fit.pareto)
exp(coef(fit.pareto))

confint(fit.pareto, level = 0.95)  # confidence intervals for model parameters 
logLik(fit.pareto)                 # loglikelihood for Pareto
AIC(fit.pareto)                    # AIC for Pareto
BIC(fit.pareto)                    # BIC for Pareto
vcov(fit.pareto)                   # covariance matrix for model parameters 
```

### Inference Assuming an Exponential Distribution
The results below assume that the data follow an exponential distribution and uses`VGAM` library for estimation of parameters. 

```{r}
fit.exp <- vglm(Claim ~ 1, exponential, data = claim_data)
summary(fit.exp)
( theta = 1 / exp(coef(fit.exp)) )

# Can also fit using the "glm" package
fit.exp2 <- glm(Claim ~ 1, data = claim_data, family = Gamma(link = log)) 
summary(fit.exp2, dispersion = 1)
( theta <- exp(coef(fit.exp2)) )  
```

### Inference Assuming a Generalized Beta Distribution of the Second Kind (GB2) Distribution
The results below assume that the data follow a GB2 distribution and uses the maximum likelihood technique for parameter estimation.
```{r}

# Inference assuming a GB2 Distribution - this is more complicated
# The likelihood functon of GB2 distribution (negative for optimization)
lik_gb2 <- function (param) {
  a_1 <- param[1]
  a_2 <- param[2]
  mu <- param[3]
  sigma <- param[4]
  yt <- (log(claim_data$Claim) - mu) / sigma
  logexpyt <- ifelse(yt > 23, yt, log(1 + exp(yt)))
  logdens <- a_1 * yt - log(sigma) - log(beta(a_1,a_2)) - 
    (a_1+a_2) * logexpyt - log(claim_data$Claim) 
  return(-sum(logdens))
}
# "optim" is a general purpose minimization function
gb2_bop <- optim(c(1, 1, 0, 1), lik_gb2, method = c("L-BFGS-B"), 
                 lower = c(0.01, 0.01, -500, 0.01), 
                 upper = c(500, 500, 500, 500), hessian = TRUE)

# Estimates
gb2_bop$par
# Standard error
sqrt(diag(solve(gb2_bop$hessian)))
# t-statistics
( tstat <- gb2_bop$par / sqrt(diag(solve(gb2_bop$hessian))) )

# density for GB II
gb2_density <- function(x){
  a_1 <- gb2_bop$par[1]
  a_2 <- gb2_bop$par[2]
  mu <- gb2_bop$par[3]
  sigma <- gb2_bop$par[4]
  xt <- (log(x) - mu) / sigma
  logexpxt<-ifelse (xt > 23, yt, log(1 + exp(xt)))
  logdens <- a_1 * xt - log(sigma) - log(beta(a_1, a_2)) - 
    (a_1+a_2) * logexpxt - log(x) 
  exp(logdens)
}

# AIC using formula : -2 * (loglik) + 2 * (number of parameters)
-2 * ( sum(log(gb2_density(claim_data$Claim))) ) + 2 * 4
# BIC using formula : -2 * (loglik) + (number of parameters) * (log(n))
-2 *( sum(log(gb2_density(claim_data$Claim))) ) + 4 * log(nrow(claim_data))
```

## Plotting the Fit Using Densities (on a Logarithmic Scale) {.tabset}

This section plots on a logarithmic scale, the smooth (nonparametric) density of claims and overlays the densities of the distributions considered above. 

```{r}
# None of these distributions is doing a great job....
plot(density(log(claim_data$Claim)), main = "", xlab = "Log Expenditures",
     ylim = c(0 ,0.37))
x <- seq(0, 15, by = 0.01)
fexp_ex <- dgamma(exp(x), scale = exp(-coef(fit.exp)), shape = 1) * exp(x)
lines(x, fexp_ex, col = "red")
fgamma_ex <- dgamma(exp(x), shape = alpha, scale = theta) * exp(x)
lines(x, fgamma_ex, col = "blue")
fpareto_ex <- dparetoII(exp(x), loc = 0, shape = exp(coef(fit.pareto)[2]), 
                        scale = exp(coef(fit.pareto)[1])) * exp(x)
lines(x, fpareto_ex, col = "purple")
flnorm_ex <- dlnorm(exp(x), mean = coef(fit.LN)[1],
                    sd = exp(coef(fit.LN)[2])) * exp(x)
lines(x, flnorm_ex, col = "lightblue")
# Density for GB II
gb2_density <- function (x) {
  a_1 <- gb2_bop$par[1]
  a_2 <- gb2_bop$par[2]
  mu <- gb2_bop$par[3]
  sigma <- gb2_bop$par[4]
  xt <- (log(x) - mu) / sigma
  logexpxt <- ifelse (xt > 23, yt, log(1 + exp(xt)))
  logdens <- a_1 * xt - log(sigma) - log(beta(a_1, a_2)) - 
    (a_1+a_2) * logexpxt -log(x) 
  exp(logdens)
  }
fGB2_ex = gb2_density(exp(x)) * exp(x)
lines(x, fGB2_ex, col="green")

legend("topleft", c("log(claim_data$Claim)", "Exponential", "Gamma", "Pareto", 
                    "lognormal", "GB2"), 
       lty = 1, col = c("black","red","blue","purple","lightblue","green"))
```

## Nonparametric Inference 

### Nonparametric Estimation Tools {.tabset}
This section illustrates non-parametric tools including moment estimators, empirical distribution function, quantiles and density estimators. 

#### Moment Estimators
The $kth$ moment $EX^k$ is estimated by $\frac{1}{n}\sum_{i=1}^{n}X_i^k$. When $k=1$ then the estimator is called the sample mean. The central moment is defined as $E(X-\mu)^k$. When $k=2$, then the central moment is called variance. Below illustrates the mean and variance. 

```{r}
# Start with a simple example of ten points
( x_example <- c(10, rep(15,3), 20, rep(23,4), 30) )

# Summary
summary(x_example)  # mean 
sd(x_example)^2  # variance 
```
#### Empirical Distribution Function 
The graph below gives the empirical distribution function `x_example` dataset.

```{r}
percentiles_x_example <- ecdf(x_example)

# Empirical distribution function
plot(percentiles_x_example, main = "", xlab = "x")
```

#### Quantiles 
The results below gives the quantiles. 

```{r}
# Quantiles 
quantile(x_example)

# Quantiles : set you own probabilities
quantile(x_example, probs = seq(0, 1, 0.333333))
# help(quantile)
```

#### Density Estimators 
The results below gives the density plots using the uniform kernel and triangular kernel. 

```{r}
# Density plot 
plot(density(x_example), main = "", xlab = "x")
plot(density(x_example, bw = .33), main = "", xlab = "x")  # change the bandwidth
plot(density(x_example, kernel = "triangular"), main="", xlab = "x")  # change the kernel
```

### Property Fund Data {.tabset}
This section employs non-parametric estimation tools for model selection for the claims data of the Property Fund.

#### Empirical Distribution Function of Property Fund
The results below gives the empirical distribution function of the claims and claims in logarithmic units. 
```{r}
claim_lev <- read.csv("DATA/CLAIMLEVEL.csv", header=TRUE)
nrow(claim_lev)  # 6258
claim_data <- subset(claim_lev, Year==2010)  # 2010 subset 
# Empirical distribution function of Property Fund
par(mfrow = c(1, 2))
percentiles  <- ecdf(claim_data$Claim)
log_percentiles  <- ecdf(log(claim_data$Claim))
plot(percentiles,  main = "", xlab = "Claims")
plot(log_percentiles, main = "", xlab = "Logarithmic Claims")
```

#### Density Comparison
Shows a histogram (with shaded gray rectangles)
of logarithmic property claims from 2010. The blue thick curve represents a Gaussian kernel density where the bandwidth was selected automatically using an ad
hoc rule based on the sample size and volatility of the data. 
```{r}
# Density comparison
hist(log(claim_data$Claim), main = "", ylim = c(0, .35), xlab = "Log Expenditures", 
     freq = FALSE, col = "lightgray")
lines(density(log(claim_data$Claim)), col = "blue", lwd = 2.5)
lines(density(log(claim_data$Claim), bw = 1), col = "green")
lines(density(log(claim_data$Claim), bw = .1), col = "red", lty = 3)
density(log(claim_data$Claim))$bw  # default bandwidth
```
### Nonparametric Estimation Tools For Model Selection {.tabset}
####Fit Distributions To The Claims Data
The results below fits gamma and Pareto distribution to the claims data. 

```{r warning = FALSE, message = FALSE}
library(MASS)
library(VGAM)
# Inference assuming a gamma distribution
fit.gamma_2 <- glm(Claim ~ 1, data = claim_data, family = Gamma(link = log)) 
summary(fit.gamma_2, dispersion = gamma.dispersion(fit.gamma_2)) 

( theta <- exp(coef(fit.gamma_2)) * gamma.dispersion(fit.gamma_2))  # mu = theta / alpha
( alpha <- 1 / gamma.dispersion(fit.gamma_2) )

# Inference assuming a Pareto distribution
fit.pareto <- vglm(Claim ~ 1, paretoII, loc = 0, data = claim_data)
summary(fit.pareto)
head(fitted(fit.pareto))
exp(coef(fit.pareto))
```

#### Graphical Comparison of Distributions
The graphs below reinforces the technique of overlaying graphs for comparison purposes using both the distribution function and density function. Pareto distribution provides a better fit.

```{r}
# Plotting the fit using densities (on a logarithmic scale)
# None of these distributions is doing a great job....
x <- seq(0, 15, by = 0.01)

par(mfrow = c(1, 2))
log_percentiles  <- ecdf(log(claim_data$Claim))
plot(log_percentiles,  main = "", xlab = "Claims", cex = 0.4)
Fgamma_ex <- pgamma(exp(x), shape = alpha, scale = theta)
lines(x, Fgamma_ex, col = "blue")
Fpareto_ex <- pparetoII(exp(x), loc = 0,shape = exp(coef(fit.pareto)[2]), 
                        scale = exp(coef(fit.pareto)[1]))
lines(x, Fpareto_ex, col = "purple")
legend("bottomright", c("log(claims)", "Gamma", "Pareto"), lty = 1, cex = 0.6, 
       col = c("black","blue","purple"))

plot(density(log(claim_data$Claim)) , main = "", xlab = "Log Expenditures")
fgamma_ex <- dgamma(exp(x), shape = alpha, scale = theta) * exp(x)
lines(x, fgamma_ex, col = "blue")
fpareto_ex <- dparetoII(exp(x), loc = 0, shape = exp(coef(fit.pareto)[2]), 
                        scale = exp(coef(fit.pareto)[1])) * exp(x)
lines(x, fpareto_ex, col = "purple")
legend("topright", c("log(claims)", "Gamma", "Pareto"), lty = 1, cex = 0.6, 
       col = c("black","blue","purple"))
```

#### P-P Plots
Shows $pp$ plots for the Property Fund data; the fitted gamma is on the left and the fitted Pareto is on the right. Pareto distribution provides a better fit again.

```{r}
# PP Plot
par(mfrow = c(1, 2))
Fgamma_ex <- pgamma(claim_data$Claim, shape = alpha, scale = theta)
plot(percentiles(claim_data$Claim), Fgamma_ex, xlab = "Empirical DF", 
     ylab = "Gamma DF", cex = 0.4)
abline(0, 1)
Fpareto_ex <- pparetoII(claim_data$Claim, loc = 0, 
                        shape = exp(coef(fit.pareto)[2]), 
                        scale = exp(coef(fit.pareto)[1]))
plot(percentiles(claim_data$Claim), Fpareto_ex, xlab = "Empirical DF", 
     ylab = "Pareto DF", cex = 0.4)
abline(0, 1)
#dev.off()
```

#### Q-Q Plots
In the graphs below the quantiles are plotted on the original scale in the left-hand panels, on the log scale in the right-hand panel, to allow the analyst to see where a fitted distribution is deficient.

```{r}
# Q-Q plot
par(mfrow = c(2, 2))
x_seq <- seq(0.0001, 0.9999, by = 1 / length(claim_data$Claim))
emp_quant <- quantile(claim_data$Claim, x_seq)
gamma_quant <- qgamma(x_seq, shape = alpha, scale = theta)
plot(emp_quant, gamma_quant, xlab = "Empirical Quantile", ylab = "Gamma Quantile")
abline(0, 1)
plot(log(emp_quant), log(gamma_quant), xlab = "Log Emp Quantile", 
     ylab = "Log Gamma Quantile")
abline(0, 1)
pareto_quant <- qparetoII(x_seq, loc = 0, shape = exp(coef(fit.pareto)[2]), 
                          scale = exp(coef(fit.pareto)[1]))
plot(emp_quant, pareto_quant, xlab = "Empirical Quantile", ylab = "Pareto Quantile")
abline(0, 1)
plot(log(emp_quant), log(pareto_quant), xlab = "Log Emp Quantile", 
     ylab="Log Pareto Quantile")
abline(0, 1)

```

#### Goodness of Fit Statistics
For reporting results, it can be effective to supplement graphical
displays with selected statistics that summarize model goodness of fit. The results below provides three commonly used goodness of fit statistics. 

```{r warning = FALSE, message = FALSE}

library(goftest)
# Kolmogorov-Smirnov # the test statistic is "D"
ks.test(claim_data$Claim, "pgamma", shape = alpha, scale = theta)
ks.test(claim_data$Claim, "pparetoII", loc = 0, shape = exp(coef(fit.pareto)[2]), 
        scale = exp(coef(fit.pareto)[1]))

# Cramer-von Mises  # the test statistic is "omega_2"
cvm.test(claim_data$Claim, "pgamma", shape = alpha, scale = theta)
cvm.test(claim_data$Claim, "pparetoII", loc = 0, shape = exp(coef(fit.pareto)[2]), 
         scale = exp(coef(fit.pareto)[1]))

# Anderson-Darling  # the test statistic is "An"
ad.test(claim_data$Claim, "pgamma", shape = alpha, scale = theta)
ad.test(claim_data$Claim, "pparetoII", loc = 0, shape = exp(coef(fit.pareto)[2]), 
        scale = exp(coef(fit.pareto)[1]))

```

##  MLE for Grouped Data {.tabset}

### MLE for Grouped Data- SOA Exam C # 276
Losses follow the distribution function $F(x)=1-(\theta/x),\quad x>0$. A sample of 20 losses resulted in the following:


 Interval         Number of Losses
----------       -------------------
 (0,10]             9
(10,25]             6
 (25,infinity)      5
 

Calculate the maximum likelihood estimate of $\theta$.
```{r}
# Log likelihood function 
lik_grp <- function (theta) {
  log_like <- log(((1 - (theta / 10))^9) * (((theta / 10) - (theta / 25))^6) *  
                   (((theta / 25))^5))
  return(-sum(log_like))
}
# "optim" is a general purpose minimization function
grp_lik <- optim(c(1), lik_grp, method = c("L-BFGS-B"), hessian = TRUE)
# Estimates - Answer "B" on SoA Problem
grp_lik$par
# Standard error
sqrt(diag(solve(grp_lik$hessian)))
# t-statistics
( tstat <- grp_lik$par / sqrt(diag(solve(grp_lik$hessian))) )

# Plot of Negative Log-Likelihood function 
vllh <- Vectorize(lik_grp, "theta")
theta <- seq(0, 10, by = 0.01)
plot(theta, vllh(theta), pch = 16, main = "Negative Log-Likelihood Function", cex = .25, 
     xlab = expression(theta), ylab = expression(paste("L(", theta, ")")))
```
