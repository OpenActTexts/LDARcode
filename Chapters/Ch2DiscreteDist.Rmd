---
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    pandoc_args: [
      "--number-sections",
      "--number-offset=1"
    ]
---
# Frequency Distributions

*This file contains illustrative **R** code for computing important count distributions. When reviewing this code, you should open an **R** session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go.*

## Basic Distributions {.tabset}

### Poisson Distribution {.tabset}

This sections shows how to compute and graph probability mass and distribution functions for the Poisson distribution.

#### Probability Mass Function (pmf) 

```{r fig.width = 6, fig.height = 4}
lambda <- 3
N <- seq(0, 20, 1)
# Get the probability mass function using "dpois"
( fn <- dpois(N, lambda) )
# Visualize the probability mass function 
plot(N, fn, xlab = "n", ylab = "f(n)") 
```

A few quick notes on these commands. 

- The assigment operator `<-` is analogous to an equal sign in mathematics. The command `lambda <- 3` means to give a value of "3" to quantity *lambda*.
- `seq` is short-hand for sequence.
- `dpois` is a built-in command in **R** for generating the "density" (actually the mass) function of the Poisson distribution. Use the online help (`help("dpois")`) to learn more about this function.
- The open paren `(`, close paren `)` tells **R** to display the output of a calculation to the screen.
- `plot` is a very handy command for displaying results graphically.

#### (Cumulative) Probability Distribution Function (cdf)
```{r  fig.width = 6, fig.height = 4}
# Get the cumulative distribution function using "ppois"
( Fn <- ppois(N, lambda) )
# Visualize the cumulative distribution function
plot(N, Fn, xlab = "n", ylab = "F(n)") # cdf
```

### Negative Binomial Distribution {.tabset}

This section shows how to compute and graph probability mass and distribution functions for the negative binomial distribution. You will also learn how to plot two functions on the same graph.

#### Probability Mass Function (pmf) {.tabset}
```{r  fig.width = 6, fig.height = 4}
alpha <- 3
theta <- 2
prob <- 1 / (1 + theta)
N <- seq(0, 30, 1)
# Get the probability mass function using "dnbinom"
( fn <- dnbinom(N, alpha, prob) )
# Visualize the probability mass function
plot(N, fn, xlab = "n", ylab = "f(n)") # pmf
```

##### Plot Two Functions on The Same Graph 
```{r  fig.width = 6, fig.height = 4}
# Plot different negative binomial distributions on the same figure
alpha_1 <- 3
alpha_2 <- 5
theta <- 2
prob <- 1 / (1 + theta)
fn_1 <- dnbinom(N, alpha_1, prob)
fn_2 <- dnbinom(N, alpha_2, prob)
plot(N, fn_1, xlab = "n", ylab = "f(n)")
lines(N, fn_2, col = "red", type = "p")
```

A couple notes on these commands: 

- You can enter more than one command on a line; separate them using the `;` semi-colon.
- `lines` is very handy for superimposing one graph on another.
- When making complex graphs with more than one function, consider using different colors. The `col = "red"` tells **R** to use the color red when plotting symbols.

#### (Cumulative) Probability Distribution Function (cdf)
```{r  fig.width = 6, fig.height = 4}
# Get the distribution function using "pnbinom"
( Fn <- pnbinom(N, alpha, prob) )
plot(N, Fn, xlab = "n", ylab = "F(n)")  # cdf
```


###  Binomial Distribution {.tabset}

This section shows how to compute and graph probability mass and distribution functions for the binomial distribution.

#### Probability Mass Function (pmf)
```{r  fig.width = 6, fig.height = 4}
# Plot different binomial distributions on the same figure
size <- 30
prob <- 0.6
N <- seq(0, 30, 1)
fn <- dbinom(N, size, prob)
plot(N, fn, xlab = "n", ylab = "f(n)")  # pdf
fn2 <- dbinom(N, size, 0.7)
lines(N, fn2, col = "red", type = "p")
```

#### (Cumulative) Probability Distribution Function (cdf)
```{r  fig.width = 6, fig.height = 4}
# Get the distribution function using "pbinom"
( Fn <- pbinom(N, size, prob) )
plot(N, Fn, xlab = "n", ylab = "F(n)")  # cdf
```

## (*a*,*b*,0) Class of Distributions {.tabset}

This section shows how to compute recursively a distribution in the (*a*,*b*,0) class. The specific example is a Poisson. However, by changing values of *a* and *b*, you can use the same recursion for negative binomial and binomial, the other two members of the 
(*a*,*b*,0) class.

```{r fig.width = 6, fig.height = 4}
lambda <- 3
a <- 0
b <- lambda
# This loop calculates the (a,b,0) recursive probabilities for the Poisson distribution
p <- rep(0, 20)
# Get the probability at n = 0 to start the recursive formula 
p[1] <- exp(-lambda)
for (i in 1:19)
  {
  p[i+1] <- (a + b / i) * p[i]  # Probability of i-th element using the ab0 formula
  }
p
# Check using the "dpois" command
dpois(seq(0, 20, 1), lambda = 3)
```

A couple notes on these commands. 

- There are many basic math commands in **R** such as `exp` for exponentials.
- This demo illustrates the use of the `for` loop, one of many ways of doing recursive calculations.


## Estimating Frequency Distributions {.tabset}

### Singapore Data
This section loads the `SingaporeAuto.csv` dataset and checks the names of variables and the dimensions of the data. To have a glimpse at the data, the first 8 observations are listed. 

```{r}
Singapore <- read.csv("Data/SingaporeAuto.csv", quote = "", header = TRUE)
# Check the names, dimensions in the file and list the first 8 observations ;
names(Singapore)
dim(Singapore)  # check number of observations and variables in the data
Singapore[1:4, ]  # list the first 4 observations
attach(Singapore)  # attach dataset
```

A few quick notes on these commands:

* The `names()`function is used to get or assign names of an object. In this illustration, it was used to get the variables names. 
* The `dim()` function is used to retrieve or set the dimensions of an object. 
* When you attach a dataset using the `attach()` function, variable names in the database can be accessed by simply giving their names.


### Claim Frequency Distribution 
The table below gives the distribution of observed claims frequency. The `Clm_Count` variable is the number of automobile accidents per policyholder. 
```{r}
table(Clm_Count) 
( n <- length(Clm_Count) )  # number of insurance policies 
```

### Visualize The Loglikelihood Function

Before maximizing, let us start by visualizing the logarithmic <a href="javascript://" title="" data-toggle="popover" data-placement="top" data-trigger="hover" data-content="A function of the parameters with the data fixed rather than a function of the data with the parameters fixed.", style="color:green;">likelihood function</a>. We will fit the claim counts for the Singapore data to the Poisson model. As an illustration, first assume that $\lambda = 0.5$. The claim count, likelihood, and its logarithmic version, for five observations is

```{r}
# Five typical observations
Clm_Count[2245:2249]
# Probabilities
dpois(Clm_Count[2245:2249], lambda = 0.5)
# Logarithmic probabilities
log(dpois(Clm_Count[2245:2249], lambda = 0.5))
```

By hand, you can check that the sum of log likelihoods for these five observations is `r sum(log(dpois(Singapore$Clm_Count[2245:2249],lambda=0.5)))`. In the same way, the sum of all `r length(Clm_Count)` observations is

```{r}
sum(log(dpois(Clm_Count, lambda = 0.5)))
```

Of course, this is only for the choice $\lambda = 0.5$. The following code defines the log likelihood to be a function of $\lambda$ and plots the function for several choices of $\lambda$:


```{r}

loglikPois <- function (parms){ 
# Defines the Poisson loglikelihood function  
  lambda = parms[1]
  llk <- sum(log(dpois(Clm_Count, lambda)))
  llk
} 
lambdax <- seq(0, .2, .01)
loglike <- 0 * lambdax
for (i in 1:length(lambdax)) 
  {
  loglike[i] <- loglikPois(lambdax[i])
}
plot(lambdax, loglike)
```

If we had to guess, from this plot we might say that the maximum value of the log likelihood was around 0.07. 

### The Maximum Likelihood Estimate of Poisson Distribution

From calculus, we know that the maximum likelihood estimator 
<a href="javascript://" title="" data-toggle="popover" data-placement="top" data-trigger="hover" data-content="Values of the parameters that are 'most likely' to have been produced by the data.", style="color:green;">(*mle*)</a> of the Poisson distribution parameter equals the average claim count. For our data, this is


```{r}
mean(Clm_Count)
```

As an alternative, let us use an optimization routine `nlminb`. Most optimization routines try to minimize functions instead of maximize them, so we first define the *negative* loglikelihood function.

```{r}
negloglikPois <- function (parms){
# Defines the (negative) Poisson loglikelihood function  
  lambda <- parms[1]
  llk <- -sum(log(dpois(Clm_Count, lambda)))
  llk
} 
ini.Pois <- 1 
zop.Pois <- nlminb(ini.Pois, negloglikPois, lower = c(1e-6), upper = c(Inf))
print(zop.Pois)  # In output, $par = MLE of lambda, $objective = - loglikelihood value
```


So, the maximum likelihood estimate, `zop.Pois$par =` `r zop.Pois$par` is exactly the same as the value that we got by hand.

Because actuarial analysts calculate Poisson mle's so regularly, here is another way of doing the calculation using the `glm`, *generalized linear model*, package.


```{r}
count_poisson1 <- glm(Clm_Count ~ 1, poisson(link = log))
summary(count_poisson1)
( lambda_hat <- exp(count_poisson1$coefficients) )
```
A few quick notes on these commands and results:

* The `glm()`function is used to fit generalized linear models. See `help(glm)` for other modeling options. In order to get the results we use the `summary() ` function.
* In the output, `call` reminds us what model we ran and what options were specified.
* The `Deviance Residuals` shows the distribution of the deviance residuals for individual cases used in the model. 
* The next part of the output shows the coefficient (maximum likelihood estimate of $\log(\lambda)$), its standard error, the z-statistic and the associated p-value. 
* To get the estimated $\lambda$ we take the $\exp$(coefficient)  `lambda_hat <- exp(count_poisson1$coefficients)`.

### The Maximum Likelihood Estimate of The Negative Binomial Distribution

In the same way, here is code for determining the maximum likelihood estimates for the negative binomial distribution.

```{r}
dnb <- function (y, r, beta){
# Defines the (negative) negative binomial loglikelihood function  
  gamma(y + r) / gamma(r) / gamma(y + 1) * (1 / (1 + beta))^r * (beta / (1 + beta))^y
}
loglikNB <- function (parms){ 
  r = parms[1]
  beta = parms[2]
  llk <- -sum(log(dnb(Clm_Count, r, beta)))
  llk
} 

ini.NB <- c(1, 1)
zop.NB <- nlminb(ini.NB, loglikNB, lower = c(1e-6, 1e-6), upper = c(Inf, Inf))
print(zop.NB)  # In output, $par = (MLE of r, MLE of beta), $objective = - loglikelihood value
```

Two quick notes:

* There are two parameters for this distribution, so that calculation by hand is not a good alternative.
* The maximum likelihood estimator of *r*, `r zop.NB$par[1]`, is not an integer.

## Goodness of Fit {.tabset}
This section shows how to check the adequacy of the Poisson and negative binomial models for the Singapore data.

First, note that the variance for the *count* data is `r var(Clm_Count)` which is greater than the mean value, `r mean(Clm_Count)`. This suggests that the negative binomial model is preferred to the Poisson model.

Second, we will compute the *Pearson goodness-of-fit statistic*.


### Pearson Goodness-of-Fit Statistic
The table below gives the distribution of fitted claims frequency using Poisson distribution $n \times p_k$

```{r}
table_1p = cbind(n * (dpois(0, lambda_hat)),
                 n * (dpois(1, lambda_hat)),
                 n * (dpois(2, lambda_hat)),
                 n * (dpois(3, lambda_hat)),
                 n * (1 - ppois(3, lambda_hat)))  
# or n * (1 - dpois(0,lambda_hat) - dpois(1, lambda_hat) -
# dpois(2, lambda_hat) - dpois(3, lambda_hat)))
actual <- data.frame(table(Clm_Count))[,2];
actual[5] <- 0  # assign 0 to claim counts greater than or equal to 4 in observed data

table_2p <- rbind(c(0, 1, 2, 3, "4+"), actual, round(table_1p, digits = 2))
rownames(table_2p) <- c("Number","Actual", "Estimated Using Poisson")
table_2p
```

For goodness of fit, consider Pearson's chi-square statistic below. The degrees of freedom (*df*) equals the number of cells minus one minus the number of estimated parameters.

```{r}
# PEARSON GOODNESS-OF-FIT STATISTIC
diff = actual - table_1p
( Pearson_p <- sum(diff * diff / table_1p) )
# p-value
1 - pchisq(Pearson_p, df = 5 - 1 - 1)
```

The large value of the goodness of fit statistic `r Pearson_p` or the small *p* value indicates that there is a large difference between actual counts and those anticipated under the Poisson model.

### Negative Binomial Goodness-of-Fit Statistic

Here is another way of determining the maximum likelihood estimator of the negative binomial distribution.


```{r}
library(MASS)
fm_nb <- glm.nb(Clm_Count ~ 1, link = log)
summary(fm_nb)
```

With these new estimates (or you could use the general procedure we introduced earlier), we can produce a table of counts and fitted counts and use this to calculate the goodness-of-fit statistic.

```{r}
fm_nb$theta
beta <- exp(fm_nb$coefficients) / fm_nb$theta
prob <- 1/(1+beta)

table_1nb = cbind(n * (dnbinom(0, size = fm_nb$theta, prob)), 
                  n * (dnbinom(1, size = fm_nb$theta, prob)), 
                  n * (dnbinom(2, size = fm_nb$theta, prob)), 
                  n * (dnbinom(3, size = fm_nb$theta, prob)),
                  n * (dnbinom(4, size = fm_nb$theta, prob)))
table_2nb <- rbind(c(0, 1, 2, 3, "4+"), actual, round(table_1nb, digits = 2))
rownames(table_2nb) <- c("Number","Actual", "Estimated Using Neg Bin")
table_2nb

# PEARSON GOODNESS-OF-FIT STATISTIC
diff = actual - table_1nb
(  Pearson_nb = sum(diff * diff / table_1nb) )
# p-value
1 - pchisq(Pearson_nb, df = 5 - 2 - 1)
```

The small value of the goodness of fit statistic `r Pearson_nb` or the high *p* value `r 1 - pchisq(Pearson_nb, df = 5 - 2 - 1)` both indicate that the negative binomial provides a better fit to the data than the Poisson.


<script>
$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

