[
["index.html", "R for Loss Data Analytics Preface", " R for Loss Data Analytics An open text authored by the Actuarial Community 2018-04-24 Preface Book Description Loss Data Analytics is an interactive, online, freely available text. The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. A subset of the book will be available for offline reading in pdf and EPUB formats. The online text will be available in multiple languages to promote access to a worldwide audience. These R codes are to support the Loss Data Analytics text. Data files are available at our GitHub site https://ewfrees.github.io/ . "],
["introduction-to-loss-data-analytics.html", "Chapter 1 Introduction to Loss Data Analytics 1.1 Case Study: Wisconsin Property Fund 1.2 Rating Variables", " Chapter 1 Introduction to Loss Data Analytics This file contains illustrative R code for computing analysis on the Property Fund data. When reviewing this code,you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go.This code uses the dataset PropertyFundInsample.csv 1.1 Case Study: Wisconsin Property Fund 1.1.1 Read in Property Fund Data Insample &lt;- read.csv(&quot;Data/PropertyFundInsample.csv&quot;, header=T, na.strings=c(&quot;.&quot;), stringsAsFactors=FALSE) Insample2010 &lt;- subset(Insample, Year==2010) A few quick notes on these commands: read.csv reads a csv file and creates data frame from it, with cases corresponding to rows and variables to columns in the file. The assignment operator &lt;- is analogous to an equal sign in mathematics. The command Insample &lt;- read.csv(&quot;PropertyFundInsample.csv&quot;, header=T, na.strings=c(&quot;.&quot;), stringsAsFactors=FALSE) means we give the name Insample to the data read. The subset() function is used to select variables and observations. In this illustration, we selected observations from year 2010. 1.1.2 Claim Frequency Distribution In 2010 there were 1,110 policyholders in the property fund. Table 1.1 shows the distribution of the 1,377 claims. 1.1.2.1 Property fund distribution for 2010 Table 1.1 library(pander) Table&lt;-as.data.frame(table(Insample2010$Freq)) names(Table)&lt;-c(&quot;Number of Claims&quot;, &quot;Frequncy&quot;) pander(t(Table)) Table continues below Number of Claims 0 1 2 3 4 5 6 7 8 9 10 11 Frequncy 707 209 86 40 18 12 9 4 6 1 3 2 Number of Claims 13 14 15 16 17 18 19 30 39 103 239 Frequncy 1 2 1 2 1 1 1 1 1 1 1 The average number of claims for this sample was 1.24 (=1377/1110). See table 1.2 below. Table 1.2 pander(summary(Insample2010$Freq)) Min. 1st Qu. Median Mean 3rd Qu. Max. 0 0 0 1.241 1 239 A few quick notes on these commands: Many useful R functions come in packages and to use these functions you have to install them. One way to install a package is by using the command line install.packages(&quot;&lt;the package's name&gt;&quot;) . In addition, to read more about a function you use the command help(&quot;function name&quot;) . The pander function is used here to create nicer tables than regular R output. To use this function you need to download the pander package. For the normal R output in the illustration above, use the command line summary(Insample2010$Freq). The names() function is used to to get or assign names of an object . In this illustration, we assigned Number of Claims and Frequency to the two columns in the data frame . The t() function is used to transpose a dataframe or a matrix. 1.1.3 Average Severity Distribution for 2010 Table 1.3 summarizes the sample distribution of average severity from the 403 policyholders; Figure 1.2 provides further information about the distribution of sample claims, showing a distribution that is dominated by this single large claim so that the histogram is not very helpful. Even when removing the large claim, you will find a distribution that is skewed to the right. A generally accepted technique is to work with claims in logarithmic units especially for graphical purposes; the corresponding figure in the right-hand panel is much easier to interpret. Table 1.3 InsamplePos2010 &lt;- subset(Insample2010, yAvg&gt;0) pander(summary(InsamplePos2010$yAvg)) Min. 1st Qu. Median Mean 3rd Qu. Max. 166.7 2226 4951 56332 11900 12922218 length(InsamplePos2010$yAvg) [1] 403 Note: The length() function sets the length of a vector (list) or other objects. 1.1.4 Plot of average claims Figure 1.2 par(mfrow=c(1, 2)) hist(InsamplePos2010$yAvg, main=&quot;&quot;, xlab=&quot;Average Claims&quot;) hist(log(InsamplePos2010$yAvg), main=&quot;&quot;, xlab=&quot;Logarithmic Average Claims&quot;) A few quick notes on these commands: The par(mfrow) function is handy for creating a simple multi-paneled plot. mfrow is a vector of length 2, where the first argument specifies the number of rows and the second the number of columns of plots. The hist() computes a histogram of the given data values. You put the name of your dataset in between the parentheses of this function 1.2 Rating Variables Earlier we considered a sample of 1,110 observations which may seem like a lot. However, as we will seen in our forthcoming applications, because of the preponderance of zeros and the skewed nature of claims, actuaries typically yearn for more data. One common approach that we adopt here is to examine outcomes from multiple years, thus increasing the sample size. Table 1.4 shows that the average claim varies over time. 1.2.1 Average claims over time Table 1.4 library(doBy) T1A &lt;- summaryBy(Freq ~ Year, data = Insample, FUN = function(x) { c(m = mean(x), num=length(x)) } ) T1B &lt;- summaryBy(yAvg ~ Year, data = Insample, FUN = function(x) { c(m = mean(x), num=length(x)) } ) T1C &lt;- summaryBy(BCcov ~ Year, data = Insample, FUN = function(x) { c(m = mean(x), num=length(x)) } ) Table1In &lt;- cbind(T1A[1],T1A[2],T1B[2],T1C[2],T1A[3]) names(Table1In) &lt;- c(&quot;Year&quot;, &quot;Average Freq&quot;,&quot;Average Sev&quot;, &quot;Average Coverage&quot;,&quot;No. of Policyholders&quot;) pander(Table1In) Year Average Freq Average Sev Average Coverage No. of Policyholders 2006 0.9515 9695 32498186 1154 2007 1.167 6544 35275949 1138 2008 0.9742 5311 37267485 1125 2009 1.219 4572 40355382 1112 2010 1.241 20452 41242070 1110 A few quick notes on these commands: The summaryBy() function provides summary statistics of a variable across different groups. You need to install the doBy package to use the command. The cbind() combines vector, matrix or data frame by columns. The row number of the two datasets must be equal. The c() function combines its arguments to form a vector. For a different look at this five-year sample, Table 1.5 summarizes the distribution of our two outcomes, frequency and claims amount. In each case, the average exceeds the median, suggesting that the distributions are right-skewed. 1.2.2 Frequency and claims statistics of full data Table 1.5 BCcov.div1000 &lt;- (Insample$BCcov)/1000 t1&lt;- summaryBy(Freq ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x),m=mean(x),mb=max(x)) } ) names(t1) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) t2 &lt;- summaryBy(yAvg ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x), m=mean(x),mb=max(x)) } ) names(t2) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) t3 &lt;- summaryBy(Deduct ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x), m=mean(x),mb=max(x)) } ) names(t3) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) t4 &lt;- summaryBy(BCcov.div1000 ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x), m=mean(x),mb=max(x)) } ) names(t4) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) Table2 &lt;- rbind(t1,t2,t3,t4) Table2a &lt;- round(Table2,3) Rowlable &lt;- rbind(&quot;Claim Frequency&quot;,&quot;Claim Severity&quot;,&quot;Deductible&quot;,&quot;Coverage (000&#39;s)&quot;) Table2aa &lt;- cbind(Rowlable,as.matrix(Table2a)) pander(Table2aa) Minimum Median Average Maximum Claim Frequency 0 0 1.109 263 Claim Severity 0 0 9291.565 12922217.84 Deductible 500 1000 3364.87 1e+05 Coverage (000’s) 8.937 11353.566 37280.855 2444796.98 A few quick notes on these commands: The rbind() combines vector, matrix or data frame by rows. The column of the two datasets must be same. The round() function rounds the values in its first argument to the specified number of decimal places (default 0). Table 1.6 describes the rating variables considered in this chapter. To handle the skewness, we henceforth focus on logarithmic transformations of coverage and deductibles. To get a sense of the relationship between the non-continuous rating variables and claims, Table 1.7 relates the claims outcomes to these categorical variables. 1.2.3 Rating variable description See table 1.6 below for variables and variable description. Table 1.6 des &lt;- read.table(header=TRUE, text=&#39; Variable Description &quot;BCcov&quot; &quot;Total building and content coverage in dollars&quot; &quot;Deduct&quot; &quot;Deductible in dollars&quot; &quot;Entity Type&quot; &quot;Categorical variable that is one of six types: (Village, City, County, Misc, School, or Town)&quot; &quot;AlarmCredit&quot; &quot;Categorical variable that is one of four types: (0%, 5%, 10%, or 15%), for automatic smoke alarms in main rooms&quot; &quot;NoClaimCredit&quot; &quot;Binary variable to indicate no claims in the past two years&quot; &quot;Fire5&quot; &quot;Binary variable to indicate the fire class is below 5. (The range of fire class is 0~10)&quot; &#39;) pander(des) Variable Description BCcov Total building and content coverage in dollars Deduct Deductible in dollars Entity Type Categorical variable that is one of six types: (Village, City, County, Misc, School, or Town) AlarmCredit Categorical variable that is one of four types: (0%, 5%, 10%, or 15%), for automatic smoke alarms in main rooms NoClaimCredit Binary variable to indicate no claims in the past two years Fire5 Binary variable to indicate the fire class is below 5. (The range of fire class is 0~10) 1.2.4 Frequency and claims by rating variables Table 1.7 shows claims summary by Entity Type, Fire Class, and No Claim Credit. Table 1.7: # Table 1.7 ByVarSumm&lt;-function(datasub){ tempA &lt;- summaryBy(Freq ~ 1 , data = datasub, FUN = function(x) { c(m = mean(x), num=length(x)) } ) datasub1&lt;- subset(datasub, yAvg&gt;0) tempB &lt;- summaryBy(yAvg ~ 1, data = datasub1, FUN = function(x) { c(m = mean(x)) } ) tempC &lt;- merge(tempA,tempB,all.x=T)[c(2,1,3)] tempC1 &lt;- as.matrix(tempC) return(tempC1) } datasub &lt;- subset(Insample, TypeVillage == 1); t1 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCity == 1); t2 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCounty == 1); t3 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeMisc == 1); t4 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeSchool == 1); t5 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeTown == 1); t6 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Fire5 == 0); t7 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Fire5 == 1); t8 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Insample$NoClaimCredit == 0); t9 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Insample$NoClaimCredit == 1); t10 &lt;- ByVarSumm(datasub) t11 &lt;- ByVarSumm(Insample) Tablea &lt;- rbind(t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11) Tableaa &lt;- round(Tablea,3) Rowlable &lt;- rbind(&quot;Village&quot;,&quot;City&quot;,&quot;County&quot;,&quot;Misc&quot;,&quot;School&quot;, &quot;Town&quot;,&quot;Fire5--No&quot;,&quot;Fire5--Yes&quot;,&quot;NoClaimCredit--No&quot;, &quot;NoClaimCredit--Yes&quot;,&quot;Total&quot;) Table4 &lt;- cbind(Rowlable,as.matrix(Tableaa)) pander(Table4) Freq.num Freq.m yAvg.m Village 1341 0.452 10645.206 City 793 1.941 16924.035 County 328 4.899 15453.206 Misc 609 0.186 43036.076 School 1597 1.434 64346.394 Town 971 0.103 19831.048 Fire5–No 2508 0.502 13935.421 Fire5–Yes 3131 1.596 41421.263 NoClaimCredit–No 3786 1.501 31365.085 NoClaimCredit–Yes 1853 0.31 30498.714 Total 5639 1.109 31206.155 Table 1.8 shows claims summary by Entity Type and Alarm Credit Table 1.8: ByVarSumm&lt;-function(datasub){ tempA &lt;- summaryBy(Freq ~ AC00 , data = datasub, FUN = function(x) { c(m = mean(x), num=length(x)) } ) datasub1 &lt;- subset(datasub, yAvg&gt;0) if(nrow(datasub1)==0) { n&lt;-nrow(datasub) return(c(0,0,n)) } else { tempB &lt;- summaryBy(yAvg ~ AC00, data = datasub1, FUN = function(x) { c(m = mean(x)) } ) tempC &lt;- merge(tempA,tempB,all.x=T)[c(2,4,3)] tempC1 &lt;- as.matrix(tempC) return(tempC1) } } AlarmC &lt;- 1*(Insample$AC00==1) + 2*(Insample$AC05==1)+ 3*(Insample$AC10==1)+ 4*(Insample$AC15==1) ByVarCredit&lt;-function(ACnum){ datasub &lt;- subset(Insample, TypeVillage == 1 &amp; AlarmC == ACnum); t1 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCity == 1 &amp; AlarmC == ACnum); t2 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCounty == 1 &amp; AlarmC == ACnum); t3 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeMisc == 1 &amp; AlarmC == ACnum); t4 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeSchool == 1 &amp; AlarmC == ACnum); t5 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeTown == 1 &amp; AlarmC ==ACnum); t6 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, AlarmC == ACnum); t7 &lt;- ByVarSumm(datasub) Tablea &lt;- rbind(t1,t2,t3,t4,t5,t6,t7) Tableaa &lt;- round(Tablea,3) Rowlable &lt;- rbind(&quot;Village&quot;,&quot;City&quot;,&quot;County&quot;,&quot;Misc&quot;,&quot;School&quot;, &quot;Town&quot;,&quot;Total&quot;) Table4 &lt;- cbind(Rowlable,as.matrix(Tableaa)) } Table4a &lt;- ByVarCredit(1) #Claims Summary by Entity Type and Alarm Credit==00 Table4b &lt;- ByVarCredit(2) #Claims Summary by Entity Type and Alarm Credit==05 Table4c &lt;- ByVarCredit(3) #Claims Summary by Entity Type and Alarm Credit==10 Table4d &lt;- ByVarCredit(4) #Claims Summary by Entity Type and Alarm Credit==15 pander(Table4a) #Claims Summary by Entity Type and Alarm Credit==00 Freq.m yAvg.m Freq.num Village 0.326 11077.997 829 City 0.893 7575.979 244 County 2.14 16012.719 50 Misc 0.117 15122.127 386 School 0.422 25522.708 294 Town 0.083 25257.084 808 Total 0.318 15118.491 2611 pander(Table4b) #Claims Summary by Entity Type and Alarm Credit==05 Freq.m yAvg.m Freq.num Village 0.278 8086.057 54 City 2.077 4150.125 13 t3 County 0 0 1 Misc 0.278 13063.933 18 School 0.41 14575.003 122 Town 0.194 3937.29 31 Total 0.431 10762.112 239 pander(Table4c) #Claims Summary by Entity Type and Alarm Credit==10 Freq.m yAvg.m Freq.num Village 0.5 8792.376 50 City 1.258 8625.169 31 County 2.125 11687.969 8 Misc 0.077 3923.375 26 School 0.488 11596.912 168 Town 0.091 2338.06 44 Total 0.517 10194.094 327 pander(Table4d) #Claims Summary by Entity Type and Alarm Credit==15 Freq.m yAvg.m Freq.num Village 0.725 10543.752 408 City 2.485 20469.514 505 County 5.513 15475.74 269 Misc 0.341 87020.878 179 School 2.008 85139.974 1013 Town 0.261 9489.613 88 Total 2.093 41458.312 2462 "],
["frequency-distributions.html", "Chapter 2 Frequency Distributions 2.1 Basic Distributions 2.2 (a,b,0) Class of Distributions 2.3 Estimating Frequency Distributions 2.4 Goodness of Fit 2.5 Singapore Data 2.6 Goodness of fit for Poisson distribution", " Chapter 2 Frequency Distributions This file contains illustrative R code for computing important count distributions. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. 2.1 Basic Distributions 2.1.1 Poisson Distribution This sections shows how to compute and graph probability mass and distribution function for the Poisson distribution. 2.1.1.1 Probability Mass Function (pmf) lambda &lt;- 3 N&lt;- seq(0,20, 1) #get the probability mass function using &quot;dpois&quot; (fn &lt;- dpois(N, lambda)) [1] 4.978707e-02 1.493612e-01 2.240418e-01 2.240418e-01 1.680314e-01 [6] 1.008188e-01 5.040941e-02 2.160403e-02 8.101512e-03 2.700504e-03 [11] 8.101512e-04 2.209503e-04 5.523758e-05 1.274713e-05 2.731529e-06 [16] 5.463057e-07 1.024323e-07 1.807629e-08 3.012715e-09 4.756919e-10 [21] 7.135379e-11 # visualize the probability mass function plot(N,fn,xlab=&quot;n&quot;,ylab=&quot;f(n)&quot;) A few quick notes on these commands. &lt;- seq is short-hand for sequence dpois is a built-in command in R for generating the “density” (actually the mass) function of the Poisson distribution. Use the online help (help(&quot;dpois&quot;)) to learn more about this function. The open paren (, close paren ) tells R to display the output of a calculation to the screen. plot is a very handy command for displaying results graphically 2.1.1.2 (Cumulative) Probability Distribution Function (cdf) #get the cumulative distribution function using &quot;ppois&quot; (Fn &lt;- ppois(N, lambda) ) [1] 0.04978707 0.19914827 0.42319008 0.64723189 0.81526324 0.91608206 [7] 0.96649146 0.98809550 0.99619701 0.99889751 0.99970766 0.99992861 [13] 0.99998385 0.99999660 0.99999933 0.99999988 0.99999998 1.00000000 [19] 1.00000000 1.00000000 1.00000000 # visualize the cumulative distribution function plot(N,Fn,xlab=&quot;n&quot;,ylab=&quot;F(n)&quot;) # cdf 2.1.2 Negative Binomial Distribution This section shows how to compute and graph probability mass and distribution function for the Poisson distribution. You will also learn how to plot two functions on the same graph. 2.1.2.1 Probability Mass Function (pmf) alpha&lt;- 3 theta&lt;- 2 prob&lt;-1/(1+theta) N&lt;- seq(0,30, 1) #get the probability mass function using &quot;dnbinom&quot; (fn &lt;- dnbinom(N, alpha,prob) ) [1] 3.703704e-02 7.407407e-02 9.876543e-02 1.097394e-01 1.097394e-01 [6] 1.024234e-01 9.104303e-02 7.803688e-02 6.503074e-02 5.298801e-02 [11] 4.239041e-02 3.339850e-02 2.597661e-02 1.998201e-02 1.522439e-02 [16] 1.150287e-02 8.627153e-03 6.428075e-03 4.761537e-03 3.508501e-03 [21] 2.572901e-03 1.878626e-03 1.366273e-03 9.900532e-04 7.150384e-04 [26] 5.148277e-04 3.696199e-04 2.646661e-04 1.890472e-04 1.347233e-04 [31] 9.580323e-05 # visualize the probability mass function plot(N,fn,xlab=&quot;n&quot;,ylab=&quot;f(n)&quot;) # pmf 2.1.2.2 (Cumulative) Probability Distribution Function (cdf) #get the distribution function using &quot;pnbinom&quot; (Fn &lt;- pnbinom(N, alpha,prob)) [1] 0.03703704 0.11111111 0.20987654 0.31961591 0.42935528 0.53177869 [7] 0.62282172 0.70085861 0.76588935 0.81887735 0.86126776 0.89466626 [13] 0.92064288 0.94062489 0.95584927 0.96735214 0.97597930 0.98240737 [19] 0.98716891 0.99067741 0.99325031 0.99512894 0.99649521 0.99748526 [25] 0.99820030 0.99871513 0.99908475 0.99934942 0.99953846 0.99967319 [31] 0.99976899 plot(N,Fn,xlab=&quot;n&quot;,ylab=&quot;F(n)&quot;) # cdf #Plot Different Negative Binomial Distributions on the same Figure alpha1 &lt;- 3 alpha2 &lt;- 5 theta &lt;- 2; prob &lt;- 1/(1+theta) fn1 &lt;- dnbinom(N, alpha1,prob) fn2 &lt;- dnbinom(N, alpha2,prob) plot(N,fn1,xlab=&quot;n&quot;, ylab=&quot;f(n)&quot;) lines(N,fn2, col=&quot;red&quot;, type=&quot;p&quot;) A couple notes on these commands. You can enter more than one command on a line; separate them using the ; semi-colon lines is very handy for superimposing one graph on another. When making complex graphs with more than one function, consider using different colors. The col=&quot;red&quot; tells R to use the color red when plotting symbols. 2.1.3 Binomial Distribution This section shows how to compute and graph probability mass and distribution function for the binomial distribution. size&lt;- 30 prob&lt;- 0.6 N&lt;- seq(0,30, 1) fn &lt;- dbinom(N,size ,prob) plot(N,fn,xlab=&quot;n&quot;,ylab=&quot;f(n)&quot;) # pdf fn2 &lt;- dbinom(N,size ,0.7) lines(N,fn2, col=&quot;red&quot;, type=&quot;p&quot;) 2.2 (a,b,0) Class of Distributions This section shows how to compute recursively a distribution in the (a,b,0) class. The specific example is a Poisson. However, by changing values of a and b, you can use the same recursion for negative binomial and binomial, the other two members of the (a,b,0) class. lambda&lt;-3 a&lt;-0 b&lt;-lambda #This loop calculates the (a,b,0) recursive probabilities for the Poisson distribution p &lt;- rep(0,20) # Get the probability at n=0 to start the recursive formula p[1]&lt;- exp(-lambda) for(i in 1:19) { p[i+1]&lt;-(a+b/i)*p[i] # Probability of i-th element using the ab0 formula } p [1] 4.978707e-02 1.493612e-01 2.240418e-01 2.240418e-01 1.680314e-01 [6] 1.008188e-01 5.040941e-02 2.160403e-02 8.101512e-03 2.700504e-03 [11] 8.101512e-04 2.209503e-04 5.523758e-05 1.274713e-05 2.731529e-06 [16] 5.463057e-07 1.024323e-07 1.807629e-08 3.012715e-09 4.756919e-10 # check using the &quot;dpois&quot; command dpois(seq(0,20, 1), lambda=3) [1] 4.978707e-02 1.493612e-01 2.240418e-01 2.240418e-01 1.680314e-01 [6] 1.008188e-01 5.040941e-02 2.160403e-02 8.101512e-03 2.700504e-03 [11] 8.101512e-04 2.209503e-04 5.523758e-05 1.274713e-05 2.731529e-06 [16] 5.463057e-07 1.024323e-07 1.807629e-08 3.012715e-09 4.756919e-10 [21] 7.135379e-11 A couple notes on these commands. There are many basic math command in R such as exp for exponentials This demo illustrates the use of the for loop, one of many ways of doing recursive calculations. 2.3 Estimating Frequency Distributions 2.3.1 Singapore Data This section loads the SingaporeAuto.csv dataset and check the name of variables and the dimension of the data. To have a glimpse at the data, the first 8 observations are listed. Singapore = read.csv(&quot;Data/SingaporeAuto.csv&quot;, quote = &quot;&quot;,header=TRUE) # Check the names, dimension in the file and list the first 8 observations ; names(Singapore) [1] &quot;SexInsured&quot; &quot;Female&quot; &quot;VehicleType&quot; &quot;PC&quot; &quot;Clm_Count&quot; [6] &quot;Exp_weights&quot; &quot;LNWEIGHT&quot; &quot;NCD&quot; &quot;AgeCat&quot; &quot;AutoAge0&quot; [11] &quot;AutoAge1&quot; &quot;AutoAge2&quot; &quot;AutoAge&quot; &quot;VAgeCat&quot; &quot;VAgecat1&quot; dim(Singapore) # check number of observations and variables in the data [1] 7483 15 Singapore[1:4,] # list the first 4 observations SexInsured Female VehicleType PC Clm_Count Exp_weights LNWEIGHT NCD 1 U 0 T 0 0 0.6680356 -0.40341383 30 2 U 0 T 0 0 0.5667351 -0.56786326 30 3 U 0 T 0 0 0.5037645 -0.68564629 30 4 U 0 T 0 0 0.9144422 -0.08944106 20 AgeCat AutoAge0 AutoAge1 AutoAge2 AutoAge VAgeCat VAgecat1 1 0 0 0 0 0 0 2 2 0 0 0 0 0 0 2 3 0 0 0 0 0 0 2 4 0 0 0 0 0 0 2 attach(Singapore) # attach dataset A few quick notes on these commands: names() The dim() function is used to retrieve or set the dimension of an object. When you attach a dataset using the attach() function, variable names in the database can be accessed by simply giving their names. 2.3.2 Claim frequency distribution The table below gives the distribution of observed claims frequency. The Clm_Count variable is the number of automobile accidents per policyholder. table(Clm_Count) Clm_Count 0 1 2 3 6996 455 28 4 (n &lt;- length(Clm_Count)) # number of insurance policies [1] 7483 2.3.3 Visualize the Loglikelihood function Before maximizing, let us start by visualizing the logarithmic likelihood function. We will fit the claim counts for the Singapore data to the Poisson model. As an illustration, first assume that \\(\\lambda = 0.5\\). The claim count, likelihood, and its logarithmic version, for five observations is # Five typical Observations Clm_Count[2245:2249] [1] 3 0 1 0 3 # Probabilities dpois(Clm_Count[2245:2249],lambda=0.5) [1] 0.01263606 0.60653066 0.30326533 0.60653066 0.01263606 # Logarithmic Probabilies log(dpois(Clm_Count[2245:2249],lambda=0.5)) [1] -4.371201 -0.500000 -1.193147 -0.500000 -4.371201 By hand, you can check that the sum of log likelihoods for these five observations is -10.9355492. In the same way, the sum of all 7483 observations is sum(log(dpois(Clm_Count,lambda=0.5))) [1] -4130.591 Of course, this is only for the choice \\(\\lambda = 0.5\\). The following code defines the log likelihood to be a function of \\(\\lambda\\) and plots the function for several choices of \\(\\lambda\\): loglikPois&lt;-function(parms){ lambda=parms[1] llk &lt;- sum(log(dpois(Clm_Count,lambda))) llk } # Defines the (negative) Poisson loglikelihood function lambdax &lt;- seq(0,.2,.01) loglike &lt;- 0*lambdax for (i in 1:length(lambdax)) { loglike[i] &lt;- loglikPois(lambdax[i]) } plot(lambdax,loglike) 2.3.4 The maximum likelihood estimate of Poisson distribution If we had to guess, from this plot we might say that the maximum value of the log likelihood was around 0.7. From calculus, we know that the maximum likelihood estimator (mle) of the Poisson distribution parameter equals the average claim count. For our data, this is mean(Clm_Count) [1] 0.06989175 As an alternative, let us use an optimization routine nlminb. Most optimization routines try to minimize functions instead of maximize them, so we first define the negative loglikelihood function. negloglikPois&lt;-function(parms){ lambda=parms[1] llk &lt;- -sum(log(dpois(Clm_Count,lambda))) llk } # Defines the (negative) Poisson loglikelihood function ini.Pois &lt;- 1 zop.Pois &lt;- nlminb(ini.Pois,negloglikPois,lower=c(1e-6),upper=c(Inf)) print(zop.Pois) # In output, $par = MLE of lambda, $objective = - loglikelihood value $par [1] 0.06989175 $objective [1] 1941.178 $convergence [1] 0 $iterations [1] 17 $evaluations function gradient 23 20 $message [1] &quot;relative convergence (4)&quot; So, the maximum likelihood estimate, zop.Pois$par = 0.0698918 is exactly the same as the value that we got by hand. Because actuarial analysts calculate Poisson mle’s so regularly, here is another way of doing the calculation using the glm, generalized linear model, package. CountPoisson1 = glm(Clm_Count ~ 1,poisson(link=log)) summary(CountPoisson1) Call: glm(formula = Clm_Count ~ 1, family = poisson(link = log)) Deviance Residuals: Min 1Q Median 3Q Max -0.3739 -0.3739 -0.3739 -0.3739 4.0861 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.66081 0.04373 -60.85 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 2887.2 on 7482 degrees of freedom Residual deviance: 2887.2 on 7482 degrees of freedom AIC: 3884.4 Number of Fisher Scoring iterations: 6 (lambda_hat&lt;-exp(CountPoisson1$coefficients)) (Intercept) 0.06989175 A few quick notes on these commands and results: The glm()function is used to fit Generalized linear models. See help(glm) for other modeling options.In order to get the results we use the summary() function. In the output, call reminds us what model we ran and what options were specified. The Deviance Residuals shows the distribution of the deviance residuals for individual cases used in the model. The next part of the output shows the coefficient (maximum likelihood estimate of \\(\\log(\\lambda)\\)), its standard error, the z-statistic and the associated p-value. To get the estimated \\(\\lambda\\) we take the \\(\\exp\\)(coefficient) lambda_hat&lt;-exp(CountPoisson1$coefficients) 2.3.5 The maximum likelihood estimate of the Negative Binomial distribution In the same way, here is code for determining the maximum likelihood estimates for the negative binomial distribution. dnb &lt;- function(y,r,beta){ gamma(y+r)/gamma(r)/gamma(y+1)*(1/(1+beta))^r*(beta/(1+beta))^y } loglikNB&lt;-function(parms){ r=parms[1] beta=parms[2] llk &lt;- -sum(log(dnb(Clm_Count,r,beta))) llk } # Defines the (negative) negative binomial loglikelihood function ini.NB &lt;- c(1,1) zop.NB &lt;- nlminb(ini.NB,loglikNB,lower=c(1e-6,1e-6),upper=c(Inf,Inf)) print(zop.NB) # In output, $par = (MLE of r, MLE of beta), $objective = - loglikelihood value $par [1] 0.87401622 0.07996624 $objective [1] 1932.383 $convergence [1] 0 $iterations [1] 24 $evaluations function gradient 30 60 $message [1] &quot;relative convergence (4)&quot; Two quick notes: There are two parameters for this distribution, so that calculation by hand is not a good alternative The maximum likelihood estimator of r, 0.8740162, is not an integer. 2.4 Goodness of Fit This section shows how to check the adequacy of the Poisson and negative binomial models for the Singapore data. First, note that the variance for the count data is 0.0757079 which is greater than the mean value, 0.0698918. This suggests that the negative binomial model is preferred to the Poisson model. Second, we will compute the Pearson goodness-of-fit statistic. 2.4.1 Pearson goodness-of-fit statistic The table below gives the distribution of fitted claims frequency using Poisson distribution \\(n \\times p_k\\) table1p = cbind(n*(dpois(0,lambda_hat)), n*(dpois(1,lambda_hat)), n*(dpois(2,lambda_hat)), n*(dpois(3,lambda_hat)), n*(1-ppois(3,lambda_hat))) # or n*(1-dpois(0,lambda_hat)-dpois(1,lambda_hat)- # dpois(2,lambda_hat)-dpois(3,lambda_hat))) actual = data.frame(table(Clm_Count))[,2]; actual[5] = 0 # assign 0 to claim counts greater than or equal to 4 in observed data table2p&lt;-rbind(c(0,1,2,3,&quot;4+&quot;),actual,round(table1p, digits = 2)) rownames(table2p) &lt;- c(&quot;Number&quot;,&quot;Actual&quot;, &quot;Estimated Using Poisson&quot;) table2p [,1] [,2] [,3] [,4] [,5] Number &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4+&quot; Actual &quot;6996&quot; &quot;455&quot; &quot;28&quot; &quot;4&quot; &quot;0&quot; Estimated Using Poisson &quot;6977.86&quot; &quot;487.69&quot; &quot;17.04&quot; &quot;0.4&quot; &quot;0.01&quot; For goodness of fit, consider Pearson’s chi-square statistic below. The degrees of freedom (df) equals the number of cells minus one minus the number of estimated parameters. # PEARSON GOODNESS-OF-FIT STATISTIC diff = actual-table1p (Pearson_p = sum(diff*diff/table1p)) [1] 41.98438 # p-value 1-pchisq(Pearson_p, df=5-1-1) [1] 4.042861e-09 The large value of the goodness of fit statistic 41.984382 or the small p value indicates that there is a large difference between actual counts and those anticipated under the Poisson model. 2.4.2 Negative binomial goodness-of-fit statistic Here is another way of determining the maximum likelihood estimator of the negative binomial distribution. library(MASS) fm_nb &lt;- glm.nb(Clm_Count~1,link=log) summary(fm_nb) Call: glm.nb(formula = Clm_Count ~ 1, link = log, init.theta = 0.8740189897) Deviance Residuals: Min 1Q Median 3Q Max -0.3667 -0.3667 -0.3667 -0.3667 3.4082 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.66081 0.04544 -58.55 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Negative Binomial(0.874) family taken to be 1) Null deviance: 2435.5 on 7482 degrees of freedom Residual deviance: 2435.5 on 7482 degrees of freedom AIC: 3868.8 Number of Fisher Scoring iterations: 1 Theta: 0.874 Std. Err.: 0.276 2 x log-likelihood: -3864.767 With these new estimates (or you could use the general procedure we introduced earlier), we can produce a table of counts and fitted counts and use this to calculate the goodness-of-fit statistic. fm_nb$theta [1] 0.874019 beta &lt;- exp(fm_nb$coefficients)/fm_nb$theta prob &lt;- 1/(1+beta) table1nb = cbind(n*(dnbinom(0,size=fm_nb$theta,prob)), n*(dnbinom(1,size=fm_nb$theta,prob)), n*(dnbinom(2,size=fm_nb$theta,prob)), n*(dnbinom(3,size=fm_nb$theta,prob)), n*(dnbinom(4,size=fm_nb$theta,prob))); table2nb&lt;-rbind(c(0,1,2,3,&quot;4+&quot;),actual,round(table1nb, digits = 2)) rownames(table2nb) &lt;- c(&quot;Number&quot;,&quot;Actual&quot;, &quot;Estimated Using Neg Bin&quot;) table2nb [,1] [,2] [,3] [,4] [,5] Number &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4+&quot; Actual &quot;6996&quot; &quot;455&quot; &quot;28&quot; &quot;4&quot; &quot;0&quot; Estimated Using Neg Bin &quot;6996.4&quot; &quot;452.78&quot; &quot;31.41&quot; &quot;2.23&quot; &quot;0.16&quot; # PEARSON GOODNESS-OF-FIT STATISTIC diff = actual-table1nb ( Pearson_nb = sum(diff*diff/table1nb) ) [1] 1.95024 # p-value 1-pchisq(Pearson_nb, df=5-2-1) [1] 0.3771472 The small value of the goodness of fit statistic 1.9502395 or the high p value 0.3771472 both indicate that the negative binomial provides a better fit to the data than the Poisson. This file contains illustrative R code for computing important count distributions. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. This code uses the dataset SingaporeAuto.csv 2.5 Singapore Data This section loads the SingaporeAuto.csv dataset and check the name of variables and the dimension of the data. To have a glimpse at the data, the first 8 observations are listed. Singapore = read.csv(&quot;data/SingaporeAuto.csv&quot;, quote = &quot;&quot;,header=TRUE) # Check the names, dimension in the file and list the first 8 observations ; names(Singapore) [1] &quot;SexInsured&quot; &quot;Female&quot; &quot;VehicleType&quot; &quot;PC&quot; &quot;Clm_Count&quot; [6] &quot;Exp_weights&quot; &quot;LNWEIGHT&quot; &quot;NCD&quot; &quot;AgeCat&quot; &quot;AutoAge0&quot; [11] &quot;AutoAge1&quot; &quot;AutoAge2&quot; &quot;AutoAge&quot; &quot;VAgeCat&quot; &quot;VAgecat1&quot; dim(Singapore) # check number of observations and variables in the data [1] 7483 15 Singapore[1:8,] # list the first 8 observations SexInsured Female VehicleType PC Clm_Count Exp_weights LNWEIGHT NCD 1 U 0 T 0 0 0.6680356 -0.40341383 30 2 U 0 T 0 0 0.5667351 -0.56786326 30 3 U 0 T 0 0 0.5037645 -0.68564629 30 4 U 0 T 0 0 0.9144422 -0.08944106 20 5 U 0 T 0 0 0.5366188 -0.62246739 20 6 U 0 T 0 0 0.7529090 -0.28381095 20 7 U 0 M 0 0 0.6707734 -0.39932384 20 8 U 0 M 0 0 0.8377823 -0.17699695 20 AgeCat AutoAge0 AutoAge1 AutoAge2 AutoAge VAgeCat VAgecat1 1 0 0 0 0 0 0 2 2 0 0 0 0 0 0 2 3 0 0 0 0 0 0 2 4 0 0 0 0 0 0 2 5 0 0 0 0 0 0 2 6 0 0 0 0 0 0 2 7 0 0 0 0 0 6 6 8 0 0 0 0 0 6 6 attach(Singapore) # attach dataset The following objects are masked from Singapore (pos = 4): AgeCat, AutoAge, AutoAge0, AutoAge1, AutoAge2, Clm_Count, Exp_weights, Female, LNWEIGHT, NCD, PC, SexInsured, VAgeCat, VAgecat1, VehicleType A few quick notes on these commands: names() dim() attach() 2.6 Goodness of fit for Poisson distribution This sections shows how to check the adequacy of the Poisson model for the Singapore data by computing the Pearson goodness-of-fit statistic. 2.6.1 Claim frequency distribution The table below gives the distribution of observed claims frequency. The Clm_Count variable is the number of automobile accidents per policyholder. table(Clm_Count) Clm_Count 0 1 2 3 6996 455 28 4 n&lt;-length(Clm_Count) # number of insurance policies n [1] 7483 2.6.2 The maximum likelihood estimate of \\(\\lambda\\) CountPoisson1 = glm(Clm_Count ~ 1,poisson(link=log)) summary(CountPoisson1) Call: glm(formula = Clm_Count ~ 1, family = poisson(link = log)) Deviance Residuals: Min 1Q Median 3Q Max -0.3739 -0.3739 -0.3739 -0.3739 4.0861 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.66081 0.04373 -60.85 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 2887.2 on 7482 degrees of freedom Residual deviance: 2887.2 on 7482 degrees of freedom AIC: 3884.4 Number of Fisher Scoring iterations: 6 lambda&lt;-exp(CountPoisson1$coefficients) lambda (Intercept) 0.06989175 A few quick notes on these commands and results: glm() help(glm) summary() call Deviance Residuals The next part of the output shows the coefficient (maximum likelihood estimate of \\(\\log(\\lambda)\\)), its standard error, the z-statistic and the associated p-value. To get the estimated \\(\\lambda\\) we take the \\(\\exp\\)(coefficient) lambda&lt;-exp(CountPoisson1$coefficients) 2.6.3 Pearson goodness-of-fit statistic The table below gives the distribution of fitted claims frequency using Poisson distribution \\((np_k)\\) table1p = cbind(n*(dpois(0,lambda)), n*(dpois(1,lambda)), n*(dpois(2,lambda)), n*(dpois(3,lambda)), n*(1-ppois(3,lambda))) # or n*(1-dpois(0,lambda)-dpois(1,lambda)-dpois(2,lambda)-dpois(3,lambda))) table2p&lt;-rbind(c(0,1,2,3,&quot;4+&quot;),round(table1p, digits = 3)) table2p [,1] [,2] [,3] [,4] [,5] [1,] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4+&quot; [2,] &quot;6977.858&quot; &quot;487.695&quot; &quot;17.043&quot; &quot;0.397&quot; &quot;0.007&quot; For goodness of fit, consider Pearson’s chi-square statistic below. The degrees of freedom (df ) equals the number of cells minus one minus the number of estimated parameters # PEARSON GOODNESS-OF-FIT STATISTIC actual = data.frame(table(Clm_Count))[,2]; actual[5] = 0 # assign 0 to claim counts greater than or equal to 4 in observed data diff = actual-table1p (PearsonG = sum(diff*diff/table1p)) [1] 41.98438 cbind(table1p,PearsonG) PearsonG [1,] 6977.858 487.6948 17.04292 0.3970532 0.007035805 41.98438 # p-value 1-pchisq(PearsonG, df=5-1-1) [1] 4.042861e-09 "],
["modeling-loss-severities.html", "Chapter 3 Modeling Loss Severities 3.1 Required packages 3.2 Gamma Distribution 3.3 Pareto Distribution 3.4 Weibull Distribution 3.5 The Generalized Beta Distribution of the Second Kind (GB2) 3.6 Methods of creating new distributions 3.7 Coverage Modifications 3.8 MLE for grouped data 3.9 Nonparametric Inference", " Chapter 3 Modeling Loss Severities This file contains illustrative R code for computing important count distributions. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. 3.1 Required packages library(actuar) library(VGAM) 3.2 Gamma Distribution This section demonstrates the effect of the shape and scale parameters on the gamma density. 3.2.1 Varying the shape parameter The graph shows the Gamma density functions with varying shape parameters \\((\\alpha)\\) # Example 1: gamma distribution # define a grid x &lt;- seq(0,1000,by=1) # define a set of scale and shape parameters scaleparam &lt;- seq(100,250,by=50) shapeparam &lt;- 2:5 # varying the shape parameter plot(x, dgamma(x, shape = shapeparam[1], scale = 100), type = &quot;l&quot;, ylab = &quot;Gamma density&quot;) for(k in 2:length(shapeparam)){ lines(x,dgamma(x,shape = shapeparam[k], scale = 100), col = k) } legend(&quot;topright&quot;, c(expression(alpha~&#39;=2&#39;), expression(alpha~&#39;=3&#39;), expression(alpha~&#39;=4&#39;), expression(alpha~&#39;=5&#39;)), lty=1, col = 1:4) title(substitute(paste(&quot;Pdf gamma density with&quot;,&quot; &quot;,theta,&quot;=100&quot;,&quot; &quot;, &quot;and varying shape&quot;))) A few quick notes on these commands : dgamma function is used for density of the Gamma distribution with shape and scale parameters . The legend function can be used to add legends to plots. plot lines 3.2.2 Varying the scale parameter The graph shows the Gamma density functions with varying scale parameters \\((\\theta)\\) plot(x, dgamma(x, shape = 2, scale = scaleparam[1]), type = &quot;l&quot;, ylab = &quot;Gamma density&quot;) for(k in 2:length(scaleparam)){ lines(x,dgamma(x,shape = 2, scale = scaleparam[k]), col = k) } legend(&quot;topright&quot;, c(expression(theta~&#39;=100&#39;), expression(theta~&#39;=150&#39;), expression(theta~&#39;=200&#39;), expression(theta~&#39;=250&#39;)), lty=1, col = 1:4) title(substitute(paste(&quot;Pdf gamma density with&quot;,&quot; &quot;,alpha,&quot;=2&quot;,&quot; &quot;, &quot;and varying scale&quot;))) knitr::include_app(&quot;https://luyang.shinyapps.io/gamma/&quot;, height = &quot;600px&quot;) 3.3 Pareto Distribution This section demonstrates the effect of the shape and scale parameters on the Pareto density function. 3.3.1 Varying the shape parameter The graph shows the Pareto density functions with varying shape parameters \\((\\alpha)\\) z&lt;- seq(0,3000,by=1) scaleparam &lt;- seq(2000,3500,500) shapeparam &lt;- 1:4 # varying the shape parameter plot(z, dparetoII(z, loc=0, shape = shapeparam[1], scale = 2000), ylim=c(0,0.002),type = &quot;l&quot;, ylab = &quot;Pareto density&quot;) for(k in 2:length(shapeparam)){ lines(z,dparetoII(z,loc=0, shape = shapeparam[k], scale = 2000), col = k) } legend(&quot;topright&quot;, c(expression(alpha~&#39;=1&#39;), expression(alpha~&#39;=2&#39;), expression(alpha~&#39;=3&#39;), expression(alpha~&#39;=4&#39;)), lty=1, col = 1:4) title(substitute(paste(&quot;Pdf Pareto density with&quot;,&quot; &quot;,theta,&quot;=2000&quot;,&quot; &quot;, &quot;and varying shape&quot;))) 3.3.2 Varying the scale parameter The graph shows the Pareto density functions with varying scale parameters \\((\\theta)\\) plot(z, dparetoII(z, loc=0, shape = 3, scale = scaleparam[1]), type = &quot;l&quot;, ylab = &quot;Pareto density&quot;) for(k in 2:length(scaleparam)){ lines(z,dparetoII(z,loc=0, shape = 3, scale = scaleparam[k]), col = k) } legend(&quot;topright&quot;, c(expression(theta~&#39;=2000&#39;), expression(theta~&#39;=2500&#39;), expression(theta~&#39;=3000&#39;), expression(theta~&#39;=3500&#39;)), lty=1, col = 1:4) title(substitute(paste(&quot;Pdf Pareto density with&quot;,&quot; &quot;,alpha,&quot;=3&quot;,&quot; &quot;, &quot;and varying scale&quot;))) 3.4 Weibull Distribution This section demonstrates the effect of the shape and scale parameters on the Weibull density function. 3.4.1 Varying the shape parameter The graph shows the Weibull density function with varying shape parameters \\((\\alpha)\\) z&lt;- seq(0,400,by=1) scaleparam &lt;- seq(50,200,50) shapeparam &lt;- seq(1.5,3,0.5) # varying the shape parameter plot(z, dweibull(z, shape = shapeparam[1], scale = 100), ylim=c(0,0.012), type = &quot;l&quot;, ylab = &quot;Weibull density&quot;) for(k in 2:length(shapeparam)){ lines(z,dweibull(z,shape = shapeparam[k], scale = 100), col = k) } legend(&quot;topright&quot;, c(expression(alpha~&#39;=1.5&#39;), expression(alpha~&#39;=2&#39;), expression(alpha~&#39;=2.5&#39;), expression(alpha~&#39;=3&#39;)), lty=1, col = 1:4) title(substitute(paste(&quot;Pdf Weibull density with&quot;,&quot; &quot;,theta,&quot;=100&quot;,&quot; &quot;, &quot;and varying shape&quot;))) 3.4.2 Varying the scale parameter The graph shows the Weibull density function with varying scale parameters \\((\\theta)\\) plot(z, dweibull(z, shape = 3, scale = scaleparam[1]), type = &quot;l&quot;, ylab = &quot;Weibull density&quot;) for(k in 2:length(scaleparam)){ lines(z,dweibull(z,shape = 3, scale = scaleparam[k]), col = k) } legend(&quot;topright&quot;, c(expression(theta~&#39;=50&#39;), expression(theta~&#39;=100&#39;), expression(theta~&#39;=150&#39;), expression(theta~&#39;=200&#39;)), lty=1, col = 1:4) title(substitute(paste(&quot;Pdf Weibull density with&quot;,&quot; &quot;,alpha,&quot;=3&quot;,&quot; &quot;, &quot;and varying scale&quot;))) 3.5 The Generalized Beta Distribution of the Second Kind (GB2) This section demonstrates the effect of the shape and scale parameters on the GB2 density function. 3.5.1 Varying the scale parameter The graph shows the GB2 density function with varying scale parameter \\((\\theta)\\) ## Example 4:GB2 gb2density &lt;- function(x,shape1,shape2,shape3,scale){ mu &lt;- log(scale) sigma &lt;- 1/shape3 xt &lt;- (log(x)-mu)/sigma logexpxt&lt;-ifelse(xt&gt;23,yt,log(1+exp(xt))) logdens &lt;- shape1*xt - log(sigma) - log(beta(shape1,shape2)) - (shape1+shape2)*logexpxt -log(x) exp(logdens) } x&lt;- seq(0,400,by=1) alpha1&lt;-5 alpha2&lt;-4 gamma &lt;-2 theta &lt;- seq(150,250,50) # varying the scale parameter plot(x, gb2density(x, shape1=alpha1,shape2=alpha2,shape3=gamma, scale = theta[1]), type = &quot;l&quot;, ylab = &quot;Gen Beta 2 density&quot;, main = expression(paste(&quot;GB2 density with &quot;, alpha[1], &quot;=5,&quot;, alpha[2], &quot;=4,&quot;, alpha[3], &quot;=2, and varying scale (&quot;,theta, &quot;) parameters&quot;)) ) for(k in 2:length(theta)){ lines(x,gb2density(x,shape1=alpha1,shape2=alpha2,shape3=gamma, scale = theta[k]), col = k) } legend(&quot;topleft&quot;, c(expression(theta~&#39;=150&#39;), expression(theta~&#39;=200&#39;), expression(theta~&#39;=250&#39;)), lty=1, cex=0.6,col = 1:3) Note: Here we wrote our own function for the density function of the GB2 density function. 3.6 Methods of creating new distributions This section shows some of the methods of creating new distributions. 3.6.1 Mixture distributions The graph below creates a density function from two random variables that follow a gamma distribution. ## Example 5: A mixed density ## specify density of a mixture of 2 gamma distributions MixtureGammaDensity &lt;- function(x, a1, a2, alphaGamma1, thetaGamma1, alphaGamma2, thetaGamma2){ a1 * dgamma(x, shape = alphaGamma1, scale = thetaGamma1) + a2 * dgamma(x, shape = alphaGamma2, scale = thetaGamma2) } w &lt;- 1:30000/100 a1&lt;-0.5 a2&lt;-0.5 alpha1 &lt;- 4 theta1 &lt;- 7 alpha2 &lt;- 15 theta2 &lt;- 7 MixGammadens &lt;- MixtureGammaDensity(w, a1,a2,alpha1, theta1, alpha2, theta2) plot(w, MixGammadens, type = &quot;l&quot;) 3.6.2 Density obtained through splicing The graph below shows a density function through splicing by combining an exponential distribution on \\((0,c)\\) with a Pareto distribution on \\((c,\\infty)\\) ##Example 6: density obtained through splicing ## combine an Exp on (0,c) with a Pareto on (c,\\infty) SpliceExpPar &lt;- function(x, c, v, theta, gamma, alpha){ if(0&lt;=x &amp; x&lt;c){return(v * dexp(x, 1/theta)/pexp(c,1/theta))}else if(x&gt;=c){return((1-v)*dparetoII(x,loc=0, shape = alpha, scale = theta)/(1-pparetoII(x,loc=0, shape = alpha, scale = theta)))} } x &lt;- t(as.matrix(1:2500/10)) spliceValues &lt;- apply(x,2,SpliceExpPar, c = 100, v = 0.6, theta = 100, gamma = 200, alpha = 4) plot(x,spliceValues, type = &#39;l&#39;) 3.7 Coverage Modifications This file contains illustrative R code for computing important count distributions. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. 3.7.1 Load required package The actuar package provides functions for dealing with coverage modifications. In the following sections we will check the functionalities of the coverage command. library(actuar) 3.7.2 Ordinary deductible This section plots the modified probability density functions due to deductibles for the payment per loss and payment per payment random variables. 3.7.2.1 Payment per loss with ordinary deductible Let \\(X\\) be the random variable for loss size. The random variable for the payment per loss with deductible \\(d\\) is \\(Y^L=(X-d)_+\\). The plot of the modified probability density function is below. f &lt;- coverage(dgamma, pgamma, deductible = 1, per.loss = TRUE)# create the object mode(f) # it&#39;s a function. Here deductible is 1 [1] &quot;function&quot; ### Check the pdf for Y^L at 0 and the original loss at 1 f(0, 3) # mass at 0 [1] 0.0803014 pgamma(0+1, 3) # idem [1] 0.0803014 curve(dgamma(x, 3), from = 0, to = 10, ylim = c(0, 0.3), lwd=1, col=&quot;gray&quot;) # original curve(dgamma(x, 3), from = 1, to = 10, ylim = c(0, 0.3), lwd=2, add=TRUE) curve(f(x, 3), from = 0.01, col = &quot;blue&quot;, add = TRUE, lwd=2) # modified points(0, f(0, 3), pch = 16, col = &quot;blue&quot;) legend(&quot;topright&quot;, c(&quot;Original pdf&quot;, &quot;Modified pdf&quot;), lty=1, cex=0.6,col = c(&quot;black&quot;,&quot;blue&quot;)) A few quick notes on these commands: The coverage() function computes probability density function or cumulative distribution function of the payment per payment or payment per loss random variable under any combination of the following coverage modifications: deductible, limit, coinsurance, inflation. In this illustration we used it to compute the probability density function of the payment per loss random variable with a deductible of 1. The f(0, 3) function calculates the pdf when the payment per loss variable is 0 with gamma parameters shape=3 and rate=1. Because we used a deductible of 1 , this should be equal to pgamma(0+1, 3). 3.7.2.2 Payment per payment with ordinary deductible \\(Y^P\\) with pdf \\(f_{Y^P}(y) = f_X(y+d)/S_X(d)\\) f &lt;- coverage(dgamma, pgamma, deductible = 1) # create the object f(0, 3) # calculate in x = 0, shape=3, rate=1 [1] 0 f(5, 3) # calculate in x = 5, shape=3, rate=1 [1] 0.04851322 dgamma(5 + 1, 3)/pgamma(1, 3, lower = FALSE) # DIY [1] 0.04851322 curve(dgamma(x, 3), from = 0, to = 10, ylim = c(0, 0.3), lwd=1,col=&quot;gray&quot;) # original pdf curve(dgamma(x, 3), from = 1, to = 10, ylim = c(0, 0.3), add=TRUE, lwd=2) curve(f(x, 3), from = 0.01, col = &quot;blue&quot;, add = TRUE,lwd=2) # modified pdf legend(&quot;topright&quot;, c(&quot;Original pdf&quot;, &quot;Modified pdf&quot;), lty=1, cex=0.6,col = c(&quot;black&quot;,&quot;blue&quot;)) 3.7.2.3 per payment variable with policy limit, coinsurance and inflation f &lt;- coverage(dgamma, pgamma, deductible = 1, limit = 100, coinsurance = 0.9, inflation = 0.05) # create the object f(0, 3) # calculate in x = 0, shape=3, rate=1 [1] 0 f(5, 3) # calculate in x = 5, shape=3, rate=1 [1] 0.0431765 curve(dgamma(x, 3), from = 0, to = 10, ylim = c(0, 0.3), lwd=1,col=&quot;gray&quot;)# original pdf curve(dgamma(x, 3), from = 1, to = 10, ylim = c(0, 0.3), add=TRUE, lwd=2) curve(f(x, 3), from = 0.01, col = &quot;blue&quot;, add = TRUE,lwd=2) # modified pdf legend(&quot;topright&quot;, c(&quot;Original pdf&quot;, &quot;Modified pdf&quot;), lty=1, cex=0.6,col = c(&quot;black&quot;,&quot;blue&quot;)) 3.7.3 Franchise deductible A policy with a franchise deductible of \\(d\\) pays nothing if the loss is no greater than \\(d\\), and pays the full amount of the loss if it is greater than \\(d\\). This section plots the pdf for the per payment and per loss random variable. 3.7.3.1 Payment per loss with franchise deductible # franchise deductible # per loss variable f &lt;- coverage(dgamma, pgamma, deductible = 1, per.loss = TRUE, franchise = TRUE) f(0, 3) # mass at 0 [1] 0.0803014 pgamma(1, 3) # idem [1] 0.0803014 f(0.5, 3) # 0 &lt; x &lt; 1 [1] 0 f(1, 3) # x = 1 [1] 0 f(5, 3) # x &gt; 1 [1] 0.08422434 dgamma(5,3) [1] 0.08422434 curve(dgamma(x, 3), from = 0, to = 10, ylim = c(0, 0.3)) # original curve(f(x, 3), from = 1.1, col = &quot;blue&quot;, add = TRUE) # modified points(0, f(0, 3), pch = 16, col = &quot;blue&quot;) # mass at 0 curve(f(x, 3), from = 0.1, to = 1, col = &quot;blue&quot;, add = TRUE) # 0 &lt; x &lt; 1 legend(&quot;topright&quot;, c(&quot;Original pdf&quot;, &quot;Modified pdf&quot;), lty=1, cex=0.6,col = c(&quot;black&quot;,&quot;blue&quot;)) Note : to use the franchise deductible , we have to add the option franchise = TRUE in the coverage function. 3.7.3.2 Payment per payment with franchise deductible # franchise deductible # per payment variable f &lt;- coverage(dgamma, pgamma, deductible = 1, franchise = TRUE) f(0, 3) # x = 0 [1] 0 f(0.5, 3) # 0 &lt; x &lt; 1 [1] 0 f(1, 3) # x = 1 [1] 0 f(5, 3) # x &gt; 1 [1] 0.09157819 dgamma(5, 3)/pgamma(1, 3, lower = FALSE) # idem [1] 0.09157819 curve(dgamma(x, 3), from = 0, to = 10, ylim = c(0, 0.3)) # original curve(f(x, 3), from = 1.1, col = &quot;blue&quot;, add = TRUE) # modified curve(f(x, 3), from = 0, to = 1, col = &quot;blue&quot;, add = TRUE) # 0 &lt; x &lt; 1 legend(&quot;topright&quot;, c(&quot;Original pdf&quot;, &quot;Modified pdf&quot;), lty=1, cex=0.6,col = c(&quot;black&quot;,&quot;blue&quot;)) 3.8 MLE for grouped data This file contains illustrative R code for computing important count distributions. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. 3.8.1 MLE for grouped data- SOA Exam C # 276 Losses follow the distribution function \\(F(x)=1-(\\theta/x),\\quad x&gt;0\\). A sample of 20 losses resulted in the following: Interval Number of Losses (0,10] 9 (10,25] 6 (25,infinity) 5 Calculate the maximum likelihood estimate of \\(\\theta\\). ##Log Likelihood function likgrp &lt;- function(theta) { loglike &lt;-log(((1-(theta/10))^9)*(((theta/10)-(theta/25))^6)* (((theta/25))^5)) return(-sum(loglike)) } # &quot;optim&quot; is a general purpose minimization function grplik &lt;- optim(c(1),likgrp,method=c(&quot;L-BFGS-B&quot;),hessian=TRUE) #Estimates - Answer &quot;B&quot; on SoA Problem grplik$par [1] 5.5 #standard error sqrt(diag(solve(grplik$hessian))) [1] 1.11243 #t-statistics (tstat = grplik$par/sqrt(diag(solve(grplik$hessian))) ) [1] 4.944132 #Plot of Negative Log-Likelihood function vllh = Vectorize(likgrp,&quot;theta&quot;) theta=seq(0,10, by=0.01) plot(theta, vllh(theta), pch=16, main =&quot;Negative Log-Likelihood function&quot; , cex=.25, xlab=expression(theta), ylab=expression(paste(&quot;L(&quot;,theta,&quot;)&quot;))) 3.9 Nonparametric Inference This file contains illustrative R code for computing important count distributions. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. This code uses the dataset CLAIMLEVEL.csv 3.9.1 Nonparametric Estimation Tools This section illustrates non-parametric tools including moment estimators, empirical distribution function, quantiles and density estimators. 3.9.1.1 Moment estimators The \\(kth\\) moment \\(EX^k\\) is estimated by \\(\\frac{1}{n}\\sum_{i=1}^{n}X_i^k\\). When \\(k=1\\) then the estimator is called the sample mean.The central moment is defined as \\(E(X-\\mu)^k\\). When \\(k=2\\), then the central moment is called variance. Below illustrates the mean and variance. # Start with a simple example of ten points (xExample = c(10,rep(15,3),20,rep(23,4),30)) [1] 10 15 15 15 20 23 23 23 23 30 ##summary summary(xExample) # mean Min. 1st Qu. Median Mean 3rd Qu. Max. 10.0 15.0 21.5 19.7 23.0 30.0 sd(xExample)^2 # variance [1] 34.45556 3.9.1.2 Empirical Distribution function The graph below gives the empirical distribution function xExample dataset. PercentilesxExample &lt;- ecdf(xExample) ###Empirical Distribution Function plot(PercentilesxExample, main=&quot;&quot;,xlab=&quot;x&quot;) 3.9.1.3 Quantiles The results below gives the quantiles. ##quantiles quantile(xExample) 0% 25% 50% 75% 100% 10.0 15.0 21.5 23.0 30.0 #quantiles : set you own probabilities quantile(xExample, probs = seq(0, 1, 0.333333)) 0% 33.3333% 66.6666% 99.9999% 10.00000 15.00000 23.00000 29.99994 #help(quantile) 3.9.1.4 Density Estimators The results below gives the density plots using the uniform kernel and triangular kernel. ##density plot plot(density(xExample), main=&quot;&quot;, xlab=&quot;x&quot;) plot(density(xExample, bw=.33), main=&quot;&quot;, xlab=&quot;x&quot;) # Change the bandwidth plot(density(xExample, kernel = &quot;triangular&quot;), main=&quot;&quot;, xlab=&quot;x&quot;) # Change the kernel 3.9.2 Property Fund Data This section employs non-parametric estimation tools for model selection for the claims data of the Property Fund. 3.9.2.1 Empirical distribution function of Property fund The results below gives the empirical distribution function of the claims and claims in logarithmic units. ClaimLev &lt;- read.csv(&quot;DATA/CLAIMLEVEL.csv&quot;, header=TRUE); nrow(ClaimLev); # 6258 [1] 6258 ClaimData&lt;-subset(ClaimLev,Year==2010); #2010 subset ##Empirical distribution function of Property fund par(mfrow=c(1, 2)) Percentiles &lt;- ecdf(ClaimData$Claim) LogPercentiles &lt;- ecdf(log(ClaimData$Claim)) plot(Percentiles, main=&quot;&quot;, xlab=&quot;Claims&quot;) plot(LogPercentiles, main=&quot;&quot;, xlab=&quot;Logarithmic Claims&quot;) 3.9.2.2 Density Comparison shows a histogram (with shaded gray rectangles) of logarithmic property claims from 2010. The blue thick curve represents a Gaussian kernel density where the bandwidth was selected automatically using an ad hoc rule based on the sample size and volatility of the data. #Density Comparison hist(log(ClaimData$Claim), main=&quot;&quot;, ylim=c(0,.35),xlab=&quot;Log Expenditures&quot;, freq=FALSE, col=&quot;lightgray&quot;) lines(density(log(ClaimData$Claim)), col=&quot;blue&quot;,lwd=2.5) lines(density(log(ClaimData$Claim), bw=1), col=&quot;green&quot;) lines(density(log(ClaimData$Claim), bw=.1), col=&quot;red&quot;, lty=3) density(log(ClaimData$Claim))$bw ##default bandwidth [1] 0.3255908 3.9.3 Nonparametric Estimation Tools For Model Selection 3.9.3.1 Fit Distributions To The Claims Data The results below fits Gamma and Pareto distribution to the claims data library(MASS) library(VGAM) # Inference assuming a gamma distribution fit.gamma2 &lt;- glm(Claim~1, data=ClaimData,family=Gamma(link=log)) summary(fit.gamma2, dispersion = gamma.dispersion(fit.gamma2)) Call: glm(formula = Claim ~ 1, family = Gamma(link = log), data = ClaimData) Deviance Residuals: Min 1Q Median 3Q Max -4.287 -2.258 -1.764 -1.178 30.926 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 10.18952 0.04999 203.8 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Gamma family taken to be 3.441204) Null deviance: 6569.1 on 1376 degrees of freedom Residual deviance: 6569.1 on 1376 degrees of freedom AIC: 28414 Number of Fisher Scoring iterations: 14 (theta&lt;-exp(coef(fit.gamma2))*gamma.dispersion(fit.gamma2)) #mu=theta/alpha (Intercept) 91613.78 (alpha&lt;-1/gamma.dispersion(fit.gamma2) ) [1] 0.2905959 # Inference assuming a Pareto Distribution fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc=0, data = ClaimData) summary(fit.pareto) Call: vglm(formula = Claim ~ 1, family = paretoII, data = ClaimData, loc = 0) Pearson residuals: Min 1Q Median 3Q Max loge(scale) -6.332 -0.8289 0.1875 0.8832 1.174 loge(shape) -10.638 0.0946 0.4047 0.4842 0.513 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 7.7329210 0.0933332 82.853 &lt;2e-16 *** (Intercept):2 -0.0008753 0.0538642 -0.016 0.987 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Number of linear predictors: 2 Names of linear predictors: loge(scale), loge(shape) Log-likelihood: -13404.64 on 2752 degrees of freedom Number of iterations: 5 head(fitted(fit.pareto)) [,1] [1,] 2285.03 [2,] 2285.03 [3,] 2285.03 [4,] 2285.03 [5,] 2285.03 [6,] 2285.03 exp(coef(fit.pareto)) (Intercept):1 (Intercept):2 2282.2590626 0.9991251 3.9.3.2 Graphical Comparison of Distributions The graphs below reinforces the technique of overlaying graphs for comparison purposes using both the distribution function and density function. Pareto distribution provides a better fit. # Plotting the fit using densities (on a logarithmic scale) # None of these distributions is doing a great job.... x &lt;- seq(0,15,by=0.01) par(mfrow=c(1, 2)) LogPercentiles &lt;- ecdf(log(ClaimData$Claim)) plot(LogPercentiles, main=&quot;&quot;, xlab=&quot;Claims&quot;, cex=0.4) Fgamma_ex = pgamma(exp(x), shape = alpha, scale=theta) lines(x,Fgamma_ex,col=&quot;blue&quot;) Fpareto_ex = pparetoII(exp(x),loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) lines(x,Fpareto_ex,col=&quot;purple&quot;) legend(&quot;bottomright&quot;, c(&quot;log(claims)&quot;, &quot;Gamma&quot;,&quot;Pareto&quot;), lty=1, cex=0.6,col = c(&quot;black&quot;,&quot;blue&quot;,&quot;purple&quot;)) plot(density(log(ClaimData$Claim)) ,main=&quot;&quot;, xlab=&quot;Log Expenditures&quot;) fgamma_ex = dgamma(exp(x), shape = alpha, scale=theta)*exp(x) lines(x,fgamma_ex,col=&quot;blue&quot;) fpareto_ex = dparetoII(exp(x),loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1]))*exp(x) lines(x,fpareto_ex,col=&quot;purple&quot;) legend(&quot;topright&quot;, c(&quot;log(claims)&quot;, &quot;Gamma&quot;,&quot;Pareto&quot;), lty=1, cex=0.6,col = c(&quot;black&quot;,&quot;blue&quot;,&quot;purple&quot;)) 3.9.3.3 P-P plots shows \\(pp\\) plots for the Property Fund data; the fitted gamma is on the left and the fitted Pareto is on the right. Pareto distribution provides a better fit again # PP Plot par(mfrow=c(1, 2)) Fgamma_ex = pgamma(ClaimData$Claim, shape = alpha, scale=theta) plot(Percentiles(ClaimData$Claim),Fgamma_ex, xlab=&quot;Empirical DF&quot;, ylab=&quot;Gamma DF&quot;,cex=0.4) abline(0,1) Fpareto_ex = pparetoII(ClaimData$Claim,loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) plot(Percentiles(ClaimData$Claim),Fpareto_ex, xlab=&quot;Empirical DF&quot;, ylab=&quot;Pareto DF&quot;,cex=0.4) abline(0,1) #dev.off() 3.9.3.4 q-q plots In the graphs below the quantiles are plotted on the original scale in the left-hand panels, on the log scale in the right-hand panel, to allow the analyst to see where a fitted distribution is deficient ##q-q plot par(mfrow=c(2, 2)) xseq = seq(0.0001, 0.9999, by=1/length(ClaimData$Claim)) empquant = quantile(ClaimData$Claim, xseq) Gammaquant = qgamma(xseq, shape = alpha, scale=theta) plot(empquant, Gammaquant, xlab=&quot;Empirical Quantile&quot;, ylab=&quot;Gamma Quantile&quot;) abline(0,1) plot(log(empquant), log(Gammaquant), xlab=&quot;Log Emp Quantile&quot;, ylab=&quot;Log Gamma Quantile&quot;) abline(0,1) Paretoquant = qparetoII(xseq,loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) plot(empquant, Paretoquant, xlab=&quot;Empirical Quantile&quot;, ylab=&quot;Pareto Quantile&quot;) abline(0,1) plot(log(empquant), log(Paretoquant), xlab=&quot;Log Emp Quantile&quot;, ylab=&quot;Log Pareto Quantile&quot;) abline(0,1) 3.9.3.5 Goodness of Fit Statistics For reporting results, it can be effective to supplement graphical displays with selected statistics that summarize model goodness of fit. The results below provides three commonly used goodness of fit statistics. library(goftest ) #Kolmogorov-Smirnov # the test statistic is &quot;D&quot; ks.test(ClaimData$Claim, &quot;pgamma&quot;, shape = alpha, scale=theta) One-sample Kolmogorov-Smirnov test data: ClaimData$Claim D = 0.26387, p-value &lt; 2.2e-16 alternative hypothesis: two-sided ks.test(ClaimData$Claim, &quot;pparetoII&quot;,loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) One-sample Kolmogorov-Smirnov test data: ClaimData$Claim D = 0.047824, p-value = 0.003677 alternative hypothesis: two-sided #Cramer-von Mises # the test statistic is &quot;omega2&quot; cvm.test(ClaimData$Claim, &quot;pgamma&quot;, shape = alpha, scale=theta) Cramer-von Mises test of goodness-of-fit Null hypothesis: Gamma distribution with parameters shape = 0.290595934110839, scale = 91613.779421033 data: ClaimData$Claim omega2 = 33.378, p-value = 2.549e-05 cvm.test(ClaimData$Claim, &quot;pparetoII&quot;,loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) Cramer-von Mises test of goodness-of-fit Null hypothesis: distribution &#39;pparetoII&#39; with parameters shape = 0.999125131378519, scale = 2282.25906257586 data: ClaimData$Claim omega2 = 0.38437, p-value = 0.07947 #Anderson-Darling # the test statistic is &quot;An&quot; ad.test(ClaimData$Claim, &quot;pgamma&quot;, shape = alpha, scale=theta) Anderson-Darling test of goodness-of-fit Null hypothesis: Gamma distribution with parameters shape = 0.290595934110839, scale = 91613.779421033 data: ClaimData$Claim An = Inf, p-value = 4.357e-07 ad.test(ClaimData$Claim, &quot;pparetoII&quot;,loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) Anderson-Darling test of goodness-of-fit Null hypothesis: distribution &#39;pparetoII&#39; with parameters shape = 0.999125131378519, scale = 2282.25906257586 data: ClaimData$Claim An = 4.1264, p-value = 0.007567 "],
["model-selection.html", "Chapter 4 Model Selection 4.1 Claim level data of Property Fund 4.2 Fitting distributions 4.3 Plotting the fit using densities (on a logarithmic scale)", " Chapter 4 Model Selection This file contains illustrative R code for computing important count distributions. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go.This code uses the dataset CLAIMLEVEL.csv 4.1 Claim level data of Property Fund This section summarizes claims from the property fund for year 2010 and plots the data. 4.1.1 Claims data The results below considers individual claims from the property fund for year 2010. ## Read in data and get number of claims. ClaimLev &lt;- read.csv(&quot;Data/CLAIMLEVEL.csv&quot;, header=TRUE); nrow(ClaimLev); # 6258 [1] 6258 #2010 subset ClaimData&lt;-subset(ClaimLev,Year==2010); length(unique(ClaimData$PolicyNum)) #403 unique policyholders [1] 403 NTot = nrow(ClaimData) #1377 individual claims NTot [1] 1377 # As an alternative, you can simulate Claims #NTot = 13770 #alphahat = 2 #thetahat = 100 #Claim = rgamma(NTot, shape = alphahat, scale = thetahat) #Claim = rparetoII(NTot, loc = 0, shape = alphahat, scale = thetahat) # GB2 #Claim = thetahat*rgamma(NTot, shape = alphahat, scale = 1)/rgamma(NTot, shape = 1, scale =1) #ClaimData &lt;- data.frame(Claim) ################################################### 4.1.2 Summary of claims The output below provides summary on claims data for 2010 and summary in logarithmic units. # Summarizing the claim data for 2010 summary(ClaimData$Claim); sd(ClaimData$Claim) Min. 1st Qu. Median Mean 3rd Qu. Max. 1 789 2250 26623 6171 12922218 [1] 368029.7 # Summarizing logarithmic claims for 2010 summary(log(ClaimData$Claim));sd(log(ClaimData$Claim)) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.000 6.670 7.719 7.804 8.728 16.374 [1] 1.683297 4.1.3 Plot of claims The plots below provides further information about the distribution of sample claims. #histogram par(mfrow=c(1, 2)) hist(ClaimData$Claim, main=&quot;&quot;, xlab=&quot;Claims&quot;) hist(log(ClaimData$Claim), main=&quot;&quot;, xlab=&quot;Logarithmic Claims&quot;) #dev.off() 4.2 Fitting distributions This section shows how to fit basic distributions to a data set. 4.2.1 Inference assuming a lognormal distribution The results below assume that the data follow a lognormal distribution and usesVGAM library for estimation of parameters # Inference assuming a lognormal distribution # First, take the log of the data and assume normality y = log(ClaimData$Claim) summary(y);sd(y) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.000 6.670 7.719 7.804 8.728 16.374 [1] 1.683297 # confidence intervals and hypothesis test t.test(y,mu=log(5000)) # H0: mu_o=log(5000)=8.517 One Sample t-test data: y t = -15.717, df = 1376, p-value &lt; 2.2e-16 alternative hypothesis: true mean is not equal to 8.517193 95 percent confidence interval: 7.715235 7.893208 sample estimates: mean of x 7.804222 #mean of the lognormal distribution exp(mean(y)+sd(y)^2/2) [1] 10106.82 mean(ClaimData$Claim) [1] 26622.59 #Alternatively, assume that the data follow a lognormal distribution #Use &quot;VGAM&quot; library for estimation of parameters library(VGAM) fit.LN &lt;- vglm(Claim ~ 1, family=lognormal, data = ClaimData) summary(fit.LN) Call: vglm(formula = Claim ~ 1, family = lognormal, data = ClaimData) Pearson residuals: Min 1Q Median 3Q Max meanlog -4.6380 -0.6740 -0.05083 0.5487 5.093 loge(sdlog) -0.7071 -0.6472 -0.44003 0.1135 17.636 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 7.80422 0.04535 172.10 &lt;2e-16 *** (Intercept):2 0.52039 0.01906 27.31 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Number of linear predictors: 2 Names of linear predictors: meanlog, loge(sdlog) Log-likelihood: -13416.87 on 2752 degrees of freedom Number of iterations: 3 coef(fit.LN) # coefficients (Intercept):1 (Intercept):2 7.8042218 0.5203908 confint(fit.LN, level=0.95) # confidence intervals for model parameters 2.5 % 97.5 % (Intercept):1 7.7153457 7.8930978 (Intercept):2 0.4830429 0.5577387 logLik(fit.LN) #loglikelihood for lognormal [1] -13416.87 AIC(fit.LN) #AIC for lognormal [1] 26837.74 BIC(fit.LN) #BIC for lognormal [1] 26848.2 vcov(fit.LN) # covariance matrix for model parameters (Intercept):1 (Intercept):2 (Intercept):1 0.002056237 0.0000000000 (Intercept):2 0.000000000 0.0003631082 #mean of the lognormal distribution exp(mean(y)+sd(y)^2/2) [1] 10106.82 exp(coef(fit.LN)) (Intercept):1 (Intercept):2 2450.927448 1.682685 A few quick notes on these commands: The t.test() function can be used for a variety of t-tests. In this illustration, it was used to test \\(H_0=\\mu_0=\\log(5000)=8.517\\) The vglm() function is used to fit vector generalized linear models (VGLMs). See help(vglm) for other modeling options. The coef() function returns the estimated coefficients from the vglm or other modeling functions. The confint function provides the confidence intervals for model parameters. The loglik function provides the log-likelihood value for the lognormal estimation from the vglm or other modeling functions. AIC() and BIC() returns Akaike’s Information Criterion and BIC or SBC (Schwarz’s Bayesian criterion) for the fitted lognormal model. \\(\\text{AIC} =-2* \\text{(loglikelihood)} + 2*\\text{npar}\\) , where npar represents the number of parameters in the fitted model, and \\(\\text{BIC} =-2* \\text{log-likelihood} + \\log(n)* \\text{npar}\\) where \\(n\\) is the number of observations. vcov() returns the covariance matrix for model parameters 4.2.2 Inference assuming a gamma distribution The results below assume that the data follow a Gamma distribution and usesVGAM library for estimation of parameters. # Inference assuming a gamma distribution #install.packages(&quot;VGAM&quot;) library(VGAM) fit.gamma &lt;- vglm(Claim ~ 1, family=gamma2, data = ClaimData) summary(fit.gamma) Call: vglm(formula = Claim ~ 1, family = gamma2, data = ClaimData) Pearson residuals: Min 1Q Median 3Q Max loge(mu) -0.539 -0.5231 -0.4935 -0.4141 261.117 loge(shape) -153.990 -0.1024 0.2335 0.4969 0.772 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 10.18952 0.04999 203.82 &lt;2e-16 *** (Intercept):2 -1.23582 0.03001 -41.17 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Number of linear predictors: 2 Names of linear predictors: loge(mu), loge(shape) Log-likelihood: -14150.59 on 2752 degrees of freedom Number of iterations: 13 coef(fit.gamma) # This uses a different parameterization (Intercept):1 (Intercept):2 10.189515 -1.235822 (theta&lt;-exp(coef(fit.gamma)[1])/exp(coef(fit.gamma)[2])) #theta=mu/alpha (Intercept):1 91613.78 (alpha&lt;-exp(coef(fit.gamma)[2])) (Intercept):2 0.2905959 plot(density(log(ClaimData$Claim)), main=&quot;&quot;, xlab=&quot;Log Expenditures&quot;) x &lt;- seq(0,15,by=0.01) fgamma_ex = dgamma(exp(x), shape = alpha, scale=theta)*exp(x) lines(x,fgamma_ex,col=&quot;blue&quot;) confint(fit.gamma, level=0.95) # confidence intervals for model parameters 2.5 % 97.5 % (Intercept):1 10.091533 10.287498 (Intercept):2 -1.294648 -1.176995 logLik(fit.gamma) #loglikelihood for gamma [1] -14150.59 AIC(fit.gamma) #AIC for gamma [1] 28305.17 BIC(fit.gamma) #BIC for gamma [1] 28315.63 vcov(fit.gamma) # covariance matrix for model parameters (Intercept):1 (Intercept):2 (Intercept):1 0.002499196 0.0000000000 (Intercept):2 0.000000000 0.0009008397 # Here is a check on the formulas #AIC using formula : -2*(loglik)+2(number of parameters) -2*(logLik(fit.gamma))+2*(length(coef(fit.gamma))) [1] 28305.17 #BIC using formula : -2*(loglik)+(number of parameters)*(log(n)) -2*(logLik(fit.gamma))+length(coef(fit.gamma, matrix = TRUE))*log(nrow(ClaimData)) [1] 28315.63 #Alternatively, we could a gamma distribution using glm library(MASS) fit.gamma2 &lt;- glm(Claim~1, data=ClaimData,family=Gamma(link=log)) summary(fit.gamma2, dispersion = gamma.dispersion(fit.gamma2)) Call: glm(formula = Claim ~ 1, family = Gamma(link = log), data = ClaimData) Deviance Residuals: Min 1Q Median 3Q Max -4.287 -2.258 -1.764 -1.178 30.926 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 10.18952 0.04999 203.8 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Gamma family taken to be 3.441204) Null deviance: 6569.1 on 1376 degrees of freedom Residual deviance: 6569.1 on 1376 degrees of freedom AIC: 28414 Number of Fisher Scoring iterations: 14 (theta&lt;-exp(coef(fit.gamma2))*gamma.dispersion(fit.gamma2)) #theta=mu/alpha (Intercept) 91613.78 (alpha&lt;-1/gamma.dispersion(fit.gamma2) ) [1] 0.2905959 logLik(fit.gamma2) #log - likelihood slightly different from vglm &#39;log Lik.&#39; -14204.77 (df=2) AIC(fit.gamma2) #AIC [1] 28413.53 BIC(fit.gamma2) #BIC [1] 28423.99 Note : The output from coef(fit.gamma) uses the parameterization \\(\\mu=\\theta * \\alpha\\). coef(fit.gamma)[1]=\\(\\log(\\mu)\\) and coef(fit.gamma)[2]=\\(\\log(\\alpha)\\),which implies , \\(\\alpha\\)=exp(coef(fit.gamma)[2]) and \\(\\theta=\\mu/\\alpha\\)=exp(coef(fit.gamma)[1])/exp(coef(fit.gamma)[2]) 4.2.3 Inference assuming a Pareto Distribution The results below assume that the data follow a Pareto distribution and usesVGAM library for estimation of parameters. fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc=0, data = ClaimData) summary(fit.pareto) Call: vglm(formula = Claim ~ 1, family = paretoII, data = ClaimData, loc = 0) Pearson residuals: Min 1Q Median 3Q Max loge(scale) -6.332 -0.8289 0.1875 0.8832 1.174 loge(shape) -10.638 0.0946 0.4047 0.4842 0.513 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 7.7329210 0.0933332 82.853 &lt;2e-16 *** (Intercept):2 -0.0008753 0.0538642 -0.016 0.987 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Number of linear predictors: 2 Names of linear predictors: loge(scale), loge(shape) Log-likelihood: -13404.64 on 2752 degrees of freedom Number of iterations: 5 head(fitted(fit.pareto)) [,1] [1,] 2285.03 [2,] 2285.03 [3,] 2285.03 [4,] 2285.03 [5,] 2285.03 [6,] 2285.03 coef(fit.pareto) (Intercept):1 (Intercept):2 7.7329210483 -0.0008752515 exp(coef(fit.pareto)) (Intercept):1 (Intercept):2 2282.2590626 0.9991251 confint(fit.pareto, level=0.95) # confidence intervals for model parameters 2.5 % 97.5 % (Intercept):1 7.5499914 7.9158507 (Intercept):2 -0.1064471 0.1046966 logLik(fit.pareto) #loglikelihood for pareto [1] -13404.64 AIC(fit.pareto) #AIC for pareto [1] 26813.29 BIC(fit.pareto) #BIC for pareto [1] 26823.74 vcov(fit.pareto) # covariance matrix for model parameters (Intercept):1 (Intercept):2 (Intercept):1 0.008711083 0.004352904 (Intercept):2 0.004352904 0.002901350 4.2.4 Inference assuming an exponential distribution The results below assume that the data follow an exponential distribution and usesVGAM library for estimation of parameters. fit.exp &lt;- vglm(Claim ~ 1, exponential, data = ClaimData) summary(fit.exp) Call: vglm(formula = Claim ~ 1, family = exponential, data = ClaimData) Pearson residuals: Min 1Q Median 3Q Max loge(rate) -484.4 0.7682 0.9155 0.9704 1 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -10.18952 0.02695 -378.1 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Number of linear predictors: 1 Name of linear predictor: loge(rate) Residual deviance: 6569.099 on 1376 degrees of freedom Log-likelihood: -15407.96 on 1376 degrees of freedom Number of iterations: 6 (theta = 1/exp(coef(fit.exp))) (Intercept) 26622.59 # Can also fit using the &quot;glm&quot; package fit.exp2 &lt;- glm(Claim~1, data=ClaimData,family=Gamma(link=log)) summary(fit.exp2,dispersion=1) Call: glm(formula = Claim ~ 1, family = Gamma(link = log), data = ClaimData) Deviance Residuals: Min 1Q Median 3Q Max -4.287 -2.258 -1.764 -1.178 30.926 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 10.18952 0.02695 378.1 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Gamma family taken to be 1) Null deviance: 6569.1 on 1376 degrees of freedom Residual deviance: 6569.1 on 1376 degrees of freedom AIC: 28414 Number of Fisher Scoring iterations: 14 (theta&lt;-exp(coef(fit.exp2))) (Intercept) 26622.59 4.2.5 Inference assuming an exponential distribution The results below assume that the data follow a GB2 distribution and uses the maximum likelihood technique for parameter estimation. # Inference assuming a GB2 Distribution - this is more complicated # The likelihood functon of GB2 distribution (negative for optimization) likgb2 &lt;- function(param) { a1 &lt;- param[1] a2 &lt;- param[2] mu &lt;- param[3] sigma &lt;- param[4] yt &lt;- (log(ClaimData$Claim)-mu)/sigma logexpyt&lt;-ifelse(yt&gt;23,yt,log(1+exp(yt))) logdens &lt;- a1*yt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpyt -log(ClaimData$Claim) return(-sum(logdens)) } # &quot;optim&quot; is a general purpose minimization function gb2bop &lt;- optim(c(1,1,0,1),likgb2,method=c(&quot;L-BFGS-B&quot;), lower=c(0.01,0.01,-500,0.01),upper=c(500,500,500,500),hessian=TRUE) #Estimates gb2bop$par [1] 2.830928 1.202500 6.328981 1.294552 #standard error sqrt(diag(solve(gb2bop$hessian))) [1] 0.9997743 0.2918469 0.3901929 0.2190362 #t-statistics (tstat = gb2bop$par/sqrt(diag(solve(gb2bop$hessian))) ) [1] 2.831567 4.120313 16.220133 5.910217 # density for GB II gb2density &lt;- function(x){ a1 &lt;- gb2bop$par[1] a2 &lt;- gb2bop$par[2] mu &lt;- gb2bop$par[3] sigma &lt;- gb2bop$par[4] xt &lt;- (log(x)-mu)/sigma logexpxt&lt;-ifelse(xt&gt;23,yt,log(1+exp(xt))) logdens &lt;- a1*xt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpxt -log(x) exp(logdens) } #AIC using formula : -2*(loglik)+2(number of parameters) -2*(sum(log(gb2density(ClaimData$Claim))))+2*4 [1] 26768.13 #BIC using formula : -2*(loglik)+(number of parameters)*(log(n)) -2*(sum(log(gb2density(ClaimData$Claim))))+4*log(nrow(ClaimData)) [1] 26789.04 4.3 Plotting the fit using densities (on a logarithmic scale) This section plots on logarithmic scale, the smooth (nonparametric) density of claims and overlay the densities the distributions considered above. # None of these distributions is doing a great job.... plot(density(log(ClaimData$Claim)), main=&quot;&quot;, xlab=&quot;Log Expenditures&quot;, ylim=c(0,0.37)) x &lt;- seq(0,15,by=0.01) fexp_ex = dgamma(exp(x), scale = exp(-coef(fit.exp)), shape = 1)*exp(x) lines(x,fexp_ex, col=&quot;red&quot;) fgamma_ex = dgamma(exp(x), shape = alpha, scale=theta)*exp(x) lines(x,fgamma_ex,col=&quot;blue&quot;) fpareto_ex = dparetoII(exp(x),loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1]))*exp(x) lines(x,fpareto_ex,col=&quot;purple&quot;) flnorm_ex = dlnorm(exp(x), mean = coef(fit.LN)[1], sd = exp(coef(fit.LN)[2]))*exp(x) lines(x,flnorm_ex, col=&quot;lightblue&quot;) # density for GB II gb2density &lt;- function(x){ a1 &lt;- gb2bop$par[1] a2 &lt;- gb2bop$par[2] mu &lt;- gb2bop$par[3] sigma &lt;- gb2bop$par[4] xt &lt;- (log(x)-mu)/sigma logexpxt&lt;-ifelse(xt&gt;23,yt,log(1+exp(xt))) logdens &lt;- a1*xt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpxt -log(x) exp(logdens) } fGB2_ex = gb2density(exp(x))*exp(x) lines(x,fGB2_ex, col=&quot;green&quot;) legend(&quot;topleft&quot;, c(&quot;log(ClaimData$Claim)&quot;,&quot;Exponential&quot;, &quot;Gamma&quot;, &quot;Pareto&quot;,&quot;lognormal&quot;,&quot;GB2&quot;), lty=1, col = c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;,&quot;purple&quot;,&quot;lightblue&quot;,&quot;green&quot;)) "],
["simulation.html", "Chapter 5 Simulation 5.1 Simulation - Inversion method 5.2 Comparing moments from the simulated data to theoretical moments", " Chapter 5 Simulation This file contains illustrative R code for computing important count distributions. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. 5.1 Simulation - Inversion method This section shows how to use the inversion method to simulate claims from a gamma distribution. The results below are summary statistics from the simulated data. ##Simulation-gamma library(moments) set.seed(2) #set seed to reproduce work nTot&lt;-20000 #number of simulations alpha&lt;-2 theta&lt;-100 Losses&lt;-rgamma(nTot,alpha,scale = theta) summary(Losses) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.0921 96.3265 167.8035 200.1747 268.2257 1110.1298 k&lt;-0.95 PercentileLoss&lt;-quantile(Losses,k) # Kth percentile of Losses PercentileLoss 95% 473.8218 ####################################### #OR you can use this method to simulate losses #Fx&lt;-runif(nTot) #Losses&lt;-qgamma(Fx,alpha,scale=theta) ####################################### # For the Pareto Distribution, use #library(VGAM) # nTot&lt;-10000 #number of simulations # alpha&lt;-3 # theta&lt;-25000 # Losses&lt;-rparetoII(nTot,scale=theta,shape=alpha) # rparetoII(nTot,scale=theta,shape=alpha) A few quick notes on these commands: The rgamma() function randomly generates data from the Gamma distribution. In this illustration the data was generated from a gamma distribution with parameters shape=alpha=2 and scale=theta=100. The quantile() function provides sample quantiles corresponding to the given probabilities. Here we wanted the simulated loss data corresponding to the 95th percentile. 5.2 Comparing moments from the simulated data to theoretical moments library(pander) ###Raw moments for k=0.5 # Theoretical k&lt;-0.5 T0.5&lt;-round(((theta^k)*gamma(alpha+k))/gamma(alpha),2) #Simulated data raw moments S0.5&lt;-round(moment(Losses, order = k, central = FALSE),2) ###Raw moments for k=1 # Theoretical k&lt;-1 T1&lt;-((theta^k)*gamma(alpha+k))/gamma(alpha) #Simulated data raw moments S1&lt;-round(moment(Losses, order = k, central = FALSE),2) ###Raw moments for k=2 # Theoretical k&lt;-2 T2&lt;-((theta^k)*gamma(alpha+k))/gamma(alpha) #Simulated data raw moments S2&lt;-round(moment(Losses, order = k, central = FALSE),2) ###Raw moments for k=3 # Theoretical k&lt;-3 T3&lt;-((theta^k)*gamma(alpha+k))/gamma(alpha) #Simulated data raw moments S3&lt;-round(moment(Losses, order = k, central = FALSE),2) ###Raw moments for k=3 # Theoretical k&lt;-4 T4&lt;-((theta^k)*gamma(alpha+k))/gamma(alpha) #Simulated data raw moments S4&lt;-round(moment(Losses, order = k, central = FALSE),2) pander(rbind(c(&quot;k&quot;,0.5,1,2,3,4),c(&quot;Theoritical&quot;,T0.5,T1,T2,T3,T4),c(&quot;Simulated&quot;,S0.5,S1,S2,S3,S4))) k 0.5 1 2 3 4 Theoritical 13.29 200 60000 2.4e+07 1.2e+10 Simulated 13.3 200.17 60069.73 23993892.53 11897735665.89 A few quick notes on these commands: The momemt() function computes all the sample moments of the chosen type up to a given order. In this illustration, we wanted the raw sample moments of the kth order The round() function was used to round values. "],
["aggregate-claim-simulation.html", "Chapter 6 Aggregate Claim Simulation 6.1 Collective Risk Model: without coverage modifications 6.2 Applications", " Chapter 6 Aggregate Claim Simulation This file demonstrates simulation of aggregate claim distributions. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. 6.1 Collective Risk Model: without coverage modifications S = X_1 + … + X_N Assume N ~ Poisson(lambda=2) and X ~ Pareto(alpha=3,theta=5000) 6.1.1 Set Parameters lambda &lt;- 2 alpha &lt;- 3 theta &lt;- 5000 6.1.2 Show frequency and severity distributions Graphing the our frequency (N) and severity (X) distributions par(mfrow=c(1,2)) n &lt;- 1:10 fn &lt;- dpois(1:10,lambda) plot(n,fn,ylim=c(0,0.3),main=&quot;Frequency: Poisson&quot;) abline(h=0,lty=2) x &lt;- seq(1,25000,1) fx &lt;- alpha*theta^alpha/(x+theta)^(alpha+1) plot(x,fx,type=&quot;l&quot;,main=&quot;Severity: Pareto&quot;) 6.1.3 Set sample size for the simulation We’re going to simulate 5000 observations of S set.seed(123) size &lt;- 5000 S &lt;- rep(NA,size) N &lt;- rpois(size,lambda) for (i in 1:size){ uu &lt;- runif(N[i]) X &lt;- theta*((1-uu)^(-1/alpha)-1) S[i] &lt;- sum(X) } 6.1.4 Show distribution of aggregate loss S par(mfrow=c(1,2)) hist(S,freq=F,breaks=100) plot(ecdf(S),xlab=&quot;S&quot;) 6.2 Applications 6.2.1 Find descriptive statistics Here we show numerical descriptions of our simulated distribution S mean(S) # sample mean [1] 4829.894 sd(S) # sample standard deviation [1] 6585.567 quantile(S,prob=c(0.05,0.5,0.95)) # percentiles 5% 50% 95% 0.000 2846.073 15983.408 6.2.2 Calculate cdf sum((S==0))/size [1] 0.1348 Pr(S=0) sum(S&lt;=mean(S))/size [1] 0.6578 Pr(S&lt;=E(S)) sum(S&gt;mean(S))/size [1] 0.3422 Pr(S&gt;E(S)) 6.2.3 Calculate risk measures CTE is also known as TVaR VaR &lt;- quantile(S,prob=0.99) # significance level = 0.01 CTE &lt;- sum(S*(S&gt;VaR))/sum((S&gt;VaR)) rm &lt;- c(VaR,CTE) names(rm) &lt;- c(&quot;VaR&quot;,&quot;CTE&quot;) print(rm) VaR CTE 28636.56 43193.19 6.2.4 Pricing stop-loss insurance - Set deductible Here we plot how the premium for a stop-loss insurance product changes based on the size of the deductible par(mfrow=c(1,1)) d &lt;- seq(0,120000,1000) price &lt;- rep(NA,length(d)) for (i in 1:length(d)){ price[i] = sum((S-d[i])*(S&gt;d[i]))/size } plot(d,price,xlab=&quot;Deductible&quot;,ylab=&quot;Stop-Loss Premium&quot;,type=&quot;b&quot;) "],
["freqsev.html", "Chapter 7 FreqSev 7.1 Getting the Data 7.2 Fit Frequency Models 7.3 Fit Severity Models", " Chapter 7 FreqSev This file contains illustrative R code for calculations involving frequency and severity of distributions. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. 7.1 Getting the Data Before we can do any analysis we must import the data. 7.1.1 read data “MassAuto.csv” Import the excel file into R. dat &lt;- read.csv(file = &quot;MassAuto.csv&quot;,header=TRUE) 7.1.2 Check Variable Names This code outputs a list of all the variable names of the excel file. This is useful for determining what kind of data you re working with. names(dat) [1] &quot;VIN&quot; &quot;Territory&quot; &quot;Class&quot; &quot;Loss1&quot; &quot;Loss2&quot; 7.1.3 Calculate Total Losses in “dat” This code creates a new column representing the sum of the two loss columns. dat$Loss &lt;- dat$Loss1 + dat$Loss2 7.2 Fit Frequency Models 7.2.1 Prepare Data for Frequency Models You may have to install the package “dplyr” to run this code. library(dplyr) freq.dat &lt;- dat %&gt;% group_by(VIN) %&gt;% summarise(tLoss = sum(Loss),count = sum(Loss&gt;0)) dim(freq.dat) [1] 49894 3 7.2.2 Fit Poisson distribution Here we fit a poisson distrubtion to the data and run log likelihood to determine the most likely parameter for the distribution. We then calculate the standard error of this estimate. 7.2.2.1 Define the pmf for the Poisson Distribution loglikPois&lt;-function(parms){ lambda=parms[1] llk &lt;- -sum(log(dpois(freq.dat$count,lambda))) llk } ini.Pois &lt;- 1 zop.Pois &lt;- nlminb(ini.Pois,loglikPois,lower=c(1e-6),upper=c(Inf)) print(zop.Pois) $par [1] 0.04475488 $objective [1] 9243.476 $convergence [1] 0 $iterations [1] 17 $evaluations function gradient 24 20 $message [1] &quot;relative convergence (4)&quot; 7.2.2.2 Obtain Standard Error library(numDeriv) est &lt;- zop.Pois$par names(est) &lt;- c(&quot;lambda&quot;) hess&lt;-hessian(loglikPois,est) se &lt;-sqrt(diag(solve(hess))) print(cbind(est,se)) est se lambda 0.04475488 0.0009471004 7.2.3 Fit Negative Binomial Distribution Now we fit a negative binomial distribution to the data using log likelihood. We then calculate the standard error of this estimate. 7.2.3.1 Define pmf for Negative Binomial dnb &lt;- function(y,r,beta){ gamma(y+r)/gamma(r)/gamma(y+1)*(1/(1+beta))^r*(beta/(1+beta))^y } loglikNB&lt;-function(parms){ r=parms[1] beta=parms[2] llk &lt;- -sum(log(dnb(freq.dat$count,r,beta))) llk } ini.NB &lt;- c(1,1) zop.NB &lt;- nlminb(ini.NB,loglikNB,lower=c(1e-6,1e-6),upper=c(Inf,Inf)) print(zop.NB) $par [1] 0.86573901 0.05169541 $objective [1] 9218.902 $convergence [1] 0 $iterations [1] 27 $evaluations function gradient 32 63 $message [1] &quot;relative convergence (4)&quot; 7.2.3.2 Obtain Standard Error library(numDeriv) est &lt;- zop.NB$par names(est) &lt;- c(&quot;r&quot;,&quot;beta&quot;) hess&lt;-hessian(loglikNB,est) se &lt;-sqrt(diag(solve(hess))) print(cbind(est,se)) est se r 0.86573901 0.161126426 beta 0.05169541 0.009686412 7.2.4 Goodness-of-Fit Here we calculate goodness of fit for the emipircal, poission, and negative binomial models. 7.2.4.1 Set Parameters lambda&lt;-zop.Pois$par r&lt;-zop.NB$par[1] beta&lt;-zop.NB$par[2] numrow&lt;-max(freq.dat$count)+1 7.2.4.2 Empirical Model emp&lt;-rep(0,numrow+1) for(i in 1:(numrow+1)){ emp[i]&lt;-sum(freq.dat$count==i-1) } 7.2.4.3 Poisson Model pois&lt;-rep(0,numrow+1) for(i in 1:numrow){ pois[i]&lt;-length(freq.dat$count)*dpois(i-1,lambda) } pois[numrow+1]&lt;- length(freq.dat$count)-sum(pois) 7.2.4.4 Negative Binomial Model nb&lt;-rep(0,numrow+1) for(i in 1:numrow){ nb[i]&lt;-length(freq.dat$count)*dnb(i-1,r,beta) } nb[numrow+1]&lt;- length(freq.dat$count)-sum(nb) 7.2.4.5 Output freq &lt;- cbind(emp,pois,nb) rownames(freq) &lt;- c(&quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;&gt;3&quot;) colnames(freq) &lt;- c(&quot;Empirical&quot;,&quot;Poisson&quot;,&quot;NegBin&quot;) round(freq,digits=3) Empirical Poisson NegBin 0 47763 47710.232 47763.629 1 2036 2135.266 2032.574 2 88 47.782 93.203 3 7 0.713 4.376 &gt;3 0 0.008 0.218 7.2.5 Chi Square Statistics Here we run chi square to determine the goodness of fit chi.pois &lt;- sum((pois-emp)^2/pois) chi.negbin &lt;- sum((nb-emp)^2/nb) chisq &lt;- c(Poisson=chi.pois, NegBin=chi.negbin) print(chisq) Poisson NegBin 93.986694 2.087543 7.3 Fit Severity Models 7.3.1 Prepare Data for Severity Models sev.dat &lt;- subset(dat,Loss&gt;0) dim(sev.dat) [1] 2233 6 7.3.2 Log-normal distribution 7.3.2.1 Use “VGAM” Library for Estimation of Parameters You may have to install the package “VGAM” to run this code. library(VGAM) fit.LN &lt;- vglm(Loss ~ 1, family=lognormal, data = sev.dat) summary(fit.LN) Call: vglm(formula = Loss ~ 1, family = lognormal, data = sev.dat) Pearson residuals: Min 1Q Median 3Q Max meanlog -4.1427 -0.4450 0.05912 0.5917 2.254 loge(sdlog) -0.7071 -0.6636 -0.51680 -0.1437 11.428 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 7.16870 0.03662 195.76 &lt;2e-16 *** (Intercept):2 0.54838 0.01496 36.65 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Number of linear predictors: 2 Names of linear predictors: meanlog, loge(sdlog) Log-likelihood: -20400.73 on 4464 degrees of freedom Number of iterations: 3 Coefficients (note scale parameter is in log scale). coef(fit.LN) (Intercept):1 (Intercept):2 7.1687024 0.5483791 Confidence intervals for model parameters. confint(fit.LN, level=0.95) 2.5 % 97.5 % (Intercept):1 7.0969291 7.2404757 (Intercept):2 0.5190507 0.5777076 Loglikelihood for lognormal. logLik(fit.LN) [1] -20400.73 AIC for lognormal. AIC(fit.LN) [1] 40805.47 BIC for lognormal. BIC(fit.LN) [1] 40816.89 Covariance matrix for model parameters. vcov(fit.LN) (Intercept):1 (Intercept):2 (Intercept):1 0.001341004 0.000000000 (Intercept):2 0.000000000 0.000223914 7.3.2.2 User-Defined Likelihood Function Here we estimate sigma directly instead of in log scale. loglikLN&lt;-function(parms){ mu=parms[1] sigma=parms[2] llk &lt;- -sum(log(dlnorm(sev.dat$Loss, mu, sigma))) llk } ini.LN &lt;- c(coef(fit.LN)[1],exp(coef(fit.LN)[2])) zop.LN &lt;- nlminb(ini.LN,loglikLN,lower=c(-Inf,1e-6),upper=c(Inf,Inf)) print(zop.LN) $par (Intercept):1 (Intercept):2 7.168702 1.730446 $objective [1] 20400.73 $convergence [1] 0 $iterations [1] 1 $evaluations function gradient 2 2 $message [1] &quot;relative convergence (4)&quot; 7.3.2.3 Obtain Standard Error library(numDeriv) est &lt;- zop.LN$par names(est) &lt;- c(&quot;mu&quot;,&quot;sigma&quot;) hess&lt;-hessian(loglikLN,est) se &lt;-sqrt(diag(solve(hess))) print(cbind(est,se)) est se mu 7.168702 0.03661961 sigma 1.730446 0.02589397 7.3.3 Pareto Distribution 7.3.3.1 Use “VGAM” Library for Estimation of Parameters You may have to install the package “VGAM” to run this code. library(VGAM) fit.pareto &lt;- vglm(Loss ~ 1, paretoII, loc=0, data = sev.dat) summary(fit.pareto) Call: vglm(formula = Loss ~ 1, family = paretoII, data = sev.dat, loc = 0) Pearson residuals: Min 1Q Median 3Q Max loge(scale) -2.044 -0.7540 0.02541 0.8477 1.2958 loge(shape) -5.813 0.1526 0.36197 0.4322 0.4559 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 7.99629 0.08417 95.004 &lt;2e-16 *** (Intercept):2 0.52653 0.05699 9.239 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Number of linear predictors: 2 Names of linear predictors: loge(scale), loge(shape) Log-likelihood: -20231.91 on 4464 degrees of freedom Number of iterations: 5 head(fitted(fit.pareto)) [,1] [1,] 1502.569 [2,] 1502.569 [3,] 1502.569 [4,] 1502.569 [5,] 1502.569 [6,] 1502.569 coef(fit.pareto) #note both parameters are in log scale (Intercept):1 (Intercept):2 7.9962932 0.5265276 exp(coef(fit.pareto)) #estimate of parameters (Intercept):1 (Intercept):2 2969.928635 1.693043 confint(fit.pareto, level=0.95) #confidence intervals for model parameters 2.5 % 97.5 % (Intercept):1 7.831327 8.1612593 (Intercept):2 0.414834 0.6382213 logLik(fit.pareto) #loglikelihood for pareto [1] -20231.91 AIC(fit.pareto) #AIC for pareto [1] 40467.83 BIC(fit.pareto) #BIC for pareto [1] 40479.25 vcov(fit.pareto) #covariance matrix for model parameters (Intercept):1 (Intercept):2 (Intercept):1 0.007084237 0.004453555 (Intercept):2 0.004453555 0.003247586 7.3.3.2 User-Defined Likelihood Function Here we estimate alpha and theta directly to define the pareto density. dpareto &lt;- function(y,theta,alpha){ alpha*theta^alpha/(y+theta)^(alpha+1) } loglikP&lt;-function(parms){ theta=parms[1] alpha=parms[2] llk &lt;- -sum(log(dpareto(sev.dat$Loss,theta,alpha))) llk } ini.P &lt;- exp(coef(fit.pareto)) zop.P &lt;- nlminb(ini.P,loglikP,lower=c(1e-6,1e-6),upper=c(Inf,Inf)) print(zop.P) $par (Intercept):1 (Intercept):2 2969.928635 1.693043 $objective [1] 20231.91 $convergence [1] 0 $iterations [1] 1 $evaluations function gradient 1 2 $message [1] &quot;both X-convergence and relative convergence (5)&quot; 7.3.3.3 Obtain Standard Error library(numDeriv) est &lt;- zop.P$par names(est) &lt;- c(&quot;theta&quot;,&quot;alpha&quot;) hess&lt;-hessian(loglikP,est) se &lt;-sqrt(diag(solve(hess))) print(cbind(est,se)) est se theta 2969.928635 248.24191162 alpha 1.693043 0.09590892 7.3.4 Histogram prepare the display window parameters to properly fit the histograms par(mfrow=c(1,2)) 7.3.4.1 LN hist(sev.dat$Loss,xlab=&quot;Total Losses&quot;,main=&quot;LN&quot;,breaks=100,freq=F,xlim=c(0,3e4),ylim=c(0,8e-4)) x &lt;- seq(1,max(sev.dat$Loss),1) mu &lt;- zop.LN$par[1] sigma &lt;- zop.LN$par[2] lines(x,dlnorm(x,mu,sigma),col=&quot;red&quot;) 7.3.4.2 Pareto hist(sev.dat$Loss,xlab=&quot;Total Losses&quot;,main=&quot;Pareto&quot;,breaks=100,freq=F,xlim=c(0,3e4),ylim=c(0,8e-4)) x &lt;- seq(1,max(sev.dat$Loss),1) theta &lt;- zop.P$par[1] alpha &lt;- zop.P$par[2] lines(x,dpareto(x,theta,alpha),col=&quot;blue&quot;) 7.3.5 qq Plots 7.3.5.1 Define Quantile Function of Pareto qpareto &lt;- function(p,theta,alpha){theta*((1-p)^(-1/alpha)-1)} pct &lt;- seq(0.01,0.99,0.01) par(mfrow=c(1,2)) plot(qlnorm(pct,mu,sigma),quantile(sev.dat$Loss,probs=pct), main=&quot;LN&quot;, xlab=&quot;Theoretical Quantile&quot;, ylab=&quot;Empirical Quantile&quot;, xlim=c(0,7.5e4),ylim=c(0,7.5e4)) abline(0,1) plot(qpareto(pct,theta,alpha),quantile(sev.dat$Loss,probs=pct), main=&quot;Pareto&quot;, xlab=&quot;Theoretical Quantile&quot;, ylab=&quot;Empirical Quantile&quot;, xlim=c(0,7.5e4),ylim=c(0,7.5e4)) abline(0,1) "],
["tweedie.html", "Chapter 8 Tweedie 8.1 Tweedie distribution", " Chapter 8 Tweedie This file contains illustrative R code for the Tweedie distribution. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. 8.1 Tweedie distribution 8.1.1 Load Tweedie Package First bring in the package Tweedie (you may need to first install this package). library(tweedie) 8.1.2 Set Paramteres for Tweedie(p,mu,phi) Setting parameters p, mu and phi defines the specific features of the distribution. Furthermore, setting a specific seed allows us to generate the same randomn numbers so we can produce identical distributions set.seed(123) p &lt;- 1.5 mu &lt;- exp(1) phi &lt;- exp(1) 8.1.3 Set Sample Size Sample size is set to 500 for this example. “y” holds all 500 obserations from tweedie distribution with the given parameters. n &lt;- 500 y &lt;- rtweedie(n,p,mu,phi) 8.1.4 Show Summary Statistics Here we calculate important statisitics like mean, median, standard deviation and quantiles. summary(y) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.000 0.000 1.438 2.687 3.878 24.181 sd(y) [1] 3.346954 quantile(y,seq(0,1,0.1)) 0% 10% 20% 30% 40% 50% 0.0000000 0.0000000 0.0000000 0.0000000 0.7275496 1.4378933 60% 70% 80% 90% 100% 2.3767214 3.4212150 5.2317625 7.7471281 24.1813833 8.1.5 Show Histogram Histograms are useful for visually interpreting data. Sometime summary statistics aren’t enough to see the full picture. hist(y, prob=T,breaks=50) x &lt;- seq(0,max(y),0.1) lines(x,dtweedie(x,p,mu,phi),col=&quot;red&quot;) 8.1.6 QQ Plots for Different p Values A QQ plot is a plot of the quantiles of the first data set against the quantiles of the second data set. This is graphical technique for determining if two data sets come from populations with a common distribution. It appears here that a power of 1.5 matches the distribution best. par(mfrow=c(2,2),mar=c(4,4,4,4)) qqTweedie &lt;- function(xi,pct,mu,phi) { plot(qtweedie(pct,xi,mu,phi),quantile(y,probs=pct), main=paste(&quot;Power = &quot;,xi), xlab=&quot;Theoretical Quantile&quot;, ylab=&quot;Empirical Quantile&quot;) abline(0,1) } pct &lt;- seq(0.01,0.99,0.01) lapply(c(1,1.5,2,2.5),qqTweedie,pct=pct,mu=mu,phi=phi) [[1]] NULL [[2]] NULL [[3]] NULL [[4]] NULL 8.1.7 Fit Tweedie Distribution Here we run a “glm” for the Tweedie distribution. you may need to first install the “statmod” package library(statmod) fit &lt;- glm(y~1,family=tweedie(var.power=1.5,link.power=0)) summary(fit) Call: glm(formula = y ~ 1, family = tweedie(var.power = 1.5, link.power = 0)) Deviance Residuals: Min 1Q Median 3Q Max -2.5607 -2.5607 -0.6876 0.5155 5.1207 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.9885 0.0557 17.75 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Tweedie family taken to be 2.542861) Null deviance: 1618.8 on 499 degrees of freedom Residual deviance: 1618.8 on 499 degrees of freedom AIC: NA Number of Fisher Scoring iterations: 4 8.1.8 Show Parameter Estimates We now display the parameter estimates calculated in the glm. summary(fit)$coefficient Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.9885415 0.0556989 17.74795 5.42159e-55 summary(fit)$dispersion [1] 2.542861 8.1.9 Maximum Likelihood Estimation Here we run a “MLE” to determine the most likely parameters of the Tweedie distribution. loglik&lt;-function(parms){ p=parms[1] mu=exp(parms[2]) phi=exp(parms[3]) llk &lt;- -sum(log(dtweedie(y, p, mu, phi))) llk } ini &lt;- c(1.5,1,1) zop &lt;- nlminb(ini,loglik, lower =c(1+1e-6,-Inf,-Inf),upper =c(2-1e-6,Inf,Inf)) print(zop) $par [1] 1.4823346 0.9885411 0.9871154 $objective [1] 1122.992 $convergence [1] 0 $iterations [1] 14 $evaluations function gradient 23 77 $message [1] &quot;relative convergence (4)&quot; 8.1.10 Obtain Standard Error Now we calculate the standard errors of our parameter estimates from the MLE. You may need to first install the “numDeriv” package. library(numDeriv) est &lt;- zop$par names(est) &lt;- c(&quot;p&quot;,&quot;mu&quot;,&quot;phi&quot;) hess&lt;-hessian(loglik,est) se &lt;-sqrt(diag(solve(hess))) print(cbind(est,se)) est se p 1.4823346 0.02260226 mu 0.9885411 0.05672086 phi 0.9871154 0.05060983 "],
["bootstrap-estimation.html", "Chapter 9 Bootstrap Estimation 9.1 Empirical Bootstrap 9.2 Parametric Bootstrap", " Chapter 9 Bootstrap Estimation This file demonstrates both empirical and parametric boostrap simulation. When reviewing this code, you should open an R session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. 9.1 Empirical Bootstrap Example: 90% confidence interval for the mean. Consider outcomes of rolling a fair die. 9.1.1 Random sample Here we input a sample of 10 observations, which we are going to build our estimates off of. y &lt;- c(1,3,2,5,4,5,5,6,6,6) n &lt;- length(y) n [1] 10 9.1.2 Sample mean Finding the mean of the random sample. xbar &lt;- mean(y) xbar [1] 4.3 9.1.3 Random resamples from y 9.1.3.1 Set bootstrap sample size We’re going to generate 30 different observations using the original sample data, called the boostrap sample nboot &lt;- 30 tmpdata &lt;- sample(y,n*nboot,replace=TRUE) bootstrap.sample &lt;- matrix(tmpdata,nrow=n,ncol=nboot) 9.1.3.2 Compute sample mean for each bootstrap sample Here we find the mean for all 30 of our boostrap samples. bsmeans &lt;- colMeans(bootstrap.sample) bsmeans [1] 3.8 4.8 4.5 4.4 4.4 4.3 4.5 4.5 5.2 4.6 3.5 4.1 4.6 4.3 4.5 4.9 4.6 [18] 4.0 4.8 4.0 4.5 4.0 4.8 4.3 4.2 4.6 3.3 3.4 4.4 3.6 9.1.4 90% confidence interval Using the generated boostrap sample, we calculate a 90% confidence interval for our sample mean. CI &lt;- c(quantile(bsmeans,prob=0.05), quantile(bsmeans,prob=0.95)) CI 5% 95% 3.445 4.855 9.2 Parametric Bootstrap Example: confidence interval for 1/theta. Consider y ~ exponential(theta=10). 9.2.1 random sample of size 250 This time we generate a random sample of 250 observations from the exponential sampling distribution. y &lt;- rexp(250,rate=0.1) n &lt;- length(y) n [1] 250 9.2.2 The MLE for lambda: 1/xbar Remember from earlier that the MLE of theta for an exponential distribution is its mean. theta.mle &lt;- mean(y) rate.hat &lt;- 1/theta.mle 9.2.3 Generate bootstrap samples Using the MLE we calculated we now generate 500 bootstrap samples of 250 observations each. nboot &lt;- 500 tmpdata &lt;- rexp(n*nboot,rate=rate.hat) bootstrap.sample &lt;- matrix(tmpdata,nrow=n,ncol=nboot) 9.2.4 compute botstrap statistics We now find the rate parameter for each of our boostrap samples rate.star &lt;- 1/colMeans(bootstrap.sample) 9.2.5 calculate deviation from sample statistics Subtracting the original sample rate parameter from each of our simulated rate parameters, we can find the 5th and 95th percentiles of the differences. delta.star &lt;- rate.star - rate.hat delta.lb &lt;- quantile(delta.star,prob=0.05) delta.ub &lt;- quantile(delta.star,prob=0.95) 9.2.6 90% confidence intervel We use the percentiles to create a 90% CI for the rate parameter. CI &lt;- rate.hat - c(delta.ub,delta.lb) print(CI) 95% 5% 0.08817793 0.10884260 "]
]
