# FreqSev

*This file contains illustrative **R** code for calculations involving frequency and severity of distributions. When reviewing this code, you should open an **R** session, copy-and-paste the code, and see it perform. Then, you will be able to change parameters, look up commands, and so forth, as you go. *

## Getting the Data {.tabset}
Before we can do any analysis we must import the data.

### read data "MassAuto.csv"
Import the excel file into R.
```{r warning=FALSE , message=FALSE , comment=""}
dat <- read.csv(file = "MassAuto.csv",header=TRUE)
```

### Check Variable Names
This code outputs a list of all the variable names of the excel file.
This is useful for determining what kind of data you re working with.
```{r warning=FALSE , message=FALSE , comment=""}
names(dat)
```
### Calculate Total Losses in "dat"
This code creates a new column representing the sum of the two loss columns.

```{r warning=FALSE , message=FALSE , comment=""}
dat$Loss <- dat$Loss1 + dat$Loss2
```

## Fit Frequency Models {.tabset}
### Prepare Data for Frequency Models
You may have to install the package "dplyr" to run this code.
```{r warning=FALSE , message=FALSE , comment=""}
library(dplyr)
freq.dat <- dat %>% group_by(VIN) %>% summarise(tLoss = sum(Loss),count = sum(Loss>0))
dim(freq.dat)
```

### Fit Poisson distribution {.tabset}
Here we fit a poisson distrubtion to the data and run log likelihood to determine the most likely parameter for the distribution.
We then calculate the standard error of this estimate.

#### Define the pmf for the Poisson Distribution
```{r warning=FALSE , message=FALSE , comment=""}
loglikPois<-function(parms){ 
  lambda=parms[1]
  llk <- -sum(log(dpois(freq.dat$count,lambda)))
  llk
}
ini.Pois <- 1
zop.Pois <- nlminb(ini.Pois,loglikPois,lower=c(1e-6),upper=c(Inf))
print(zop.Pois)
```

#### Obtain Standard Error
```{r warning=FALSE , message=FALSE , comment=""}
library(numDeriv)
est <- zop.Pois$par
names(est) <- c("lambda")
hess<-hessian(loglikPois,est)
se <-sqrt(diag(solve(hess)))
print(cbind(est,se))
```

### Fit Negative Binomial Distribution {.tabset}
Now we fit a negative binomial distribution to the data using log likelihood.

We then calculate the standard error of this estimate.

#### Define pmf for Negative Binomial
```{r warning=FALSE , message=FALSE , comment=""}
dnb <- function(y,r,beta){
  gamma(y+r)/gamma(r)/gamma(y+1)*(1/(1+beta))^r*(beta/(1+beta))^y
}
loglikNB<-function(parms){ 
  r=parms[1]
  beta=parms[2]
  llk <- -sum(log(dnb(freq.dat$count,r,beta)))
  llk
}
ini.NB <- c(1,1)
zop.NB <- nlminb(ini.NB,loglikNB,lower=c(1e-6,1e-6),upper=c(Inf,Inf))
print(zop.NB)
```

#### Obtain Standard Error
```{r warning=FALSE , message=FALSE , comment=""}
library(numDeriv)
est <- zop.NB$par
names(est) <- c("r","beta")
hess<-hessian(loglikNB,est)
se <-sqrt(diag(solve(hess)))
print(cbind(est,se))
```
### Goodness-of-Fit{.tabset}
Here we calculate goodness of fit for the emipircal, poission, and negative binomial models. 

#### Set Parameters
```{r warning=FALSE , message=FALSE , comment=""}
lambda<-zop.Pois$par 
r<-zop.NB$par[1]
beta<-zop.NB$par[2]
numrow<-max(freq.dat$count)+1
```

#### Empirical Model
```{r warning=FALSE , message=FALSE , comment=""}
emp<-rep(0,numrow+1)
for(i in 1:(numrow+1)){
  emp[i]<-sum(freq.dat$count==i-1)
}
```
#### Poisson Model
```{r warning=FALSE , message=FALSE , comment=""}
pois<-rep(0,numrow+1)
for(i in 1:numrow){
  pois[i]<-length(freq.dat$count)*dpois(i-1,lambda)
}
pois[numrow+1]<- length(freq.dat$count)-sum(pois)
```
#### Negative Binomial Model
```{r warning=FALSE , message=FALSE , comment=""}
nb<-rep(0,numrow+1)
for(i in 1:numrow){
  nb[i]<-length(freq.dat$count)*dnb(i-1,r,beta)
}
nb[numrow+1]<- length(freq.dat$count)-sum(nb)
```

#### Output
```{r warning=FALSE , message=FALSE , comment=""}
freq <- cbind(emp,pois,nb)
rownames(freq) <- c("0","1","2","3",">3")
colnames(freq) <- c("Empirical","Poisson","NegBin")
round(freq,digits=3)
```

### Chi Square Statistics
Here we run chi square to determine the goodness of fit
```{r warning=FALSE , message=FALSE , comment=""}
chi.pois <- sum((pois-emp)^2/pois)
chi.negbin <- sum((nb-emp)^2/nb)
chisq <- c(Poisson=chi.pois, NegBin=chi.negbin)
print(chisq)
```




## Fit Severity Models{.tabset}

### Prepare Data for Severity Models
```{r warning=FALSE , message=FALSE , comment=""}
sev.dat <- subset(dat,Loss>0)
dim(sev.dat)
```

### Log-normal distribution{.tabset}
#### Use "VGAM" Library for Estimation of Parameters 
You may have to install the package "VGAM" to run this code.
```{r warning=FALSE , message=FALSE , comment=""}
library(VGAM)
fit.LN <- vglm(Loss ~ 1, family=lognormal, data = sev.dat)
summary(fit.LN)
```
 Coefficients (note scale parameter is in log scale).
```{r warning=FALSE , message=FALSE , comment=""}
coef(fit.LN)                
```
Confidence intervals for model parameters.
```{r warning=FALSE , message=FALSE , comment=""}
confint(fit.LN, level=0.95)   
```
Loglikelihood for lognormal.
```{r warning=FALSE , message=FALSE , comment=""}
logLik(fit.LN)               
```
AIC for lognormal.
```{r warning=FALSE , message=FALSE , comment=""}
AIC(fit.LN)                 
```
BIC for lognormal.
```{r warning=FALSE , message=FALSE , comment=""}
BIC(fit.LN)                 
```
Covariance matrix for model parameters. 
```{r warning=FALSE , message=FALSE , comment=""}
vcov(fit.LN)                 
```

#### User-Defined Likelihood Function
Here we estimate sigma directly instead of in log scale.
```{r warning=FALSE , message=FALSE , comment=""}
loglikLN<-function(parms){ 
  mu=parms[1]
  sigma=parms[2]
  llk <- -sum(log(dlnorm(sev.dat$Loss, mu, sigma)))
  llk
}
ini.LN <- c(coef(fit.LN)[1],exp(coef(fit.LN)[2]))
zop.LN <- nlminb(ini.LN,loglikLN,lower=c(-Inf,1e-6),upper=c(Inf,Inf))
print(zop.LN)
```

#### Obtain Standard Error
```{r warning=FALSE , message=FALSE , comment=""}
library(numDeriv)
est <- zop.LN$par
names(est) <- c("mu","sigma")
hess<-hessian(loglikLN,est)
se <-sqrt(diag(solve(hess)))
print(cbind(est,se))
```


### Pareto Distribution{.tabset}
#### Use "VGAM" Library for Estimation of Parameters
You may have to install the package "VGAM" to run this code.
```{r warning=FALSE , message=FALSE , comment=""}
library(VGAM)
fit.pareto <- vglm(Loss ~ 1, paretoII, loc=0, data = sev.dat)
summary(fit.pareto)
head(fitted(fit.pareto))
coef(fit.pareto)                 #note both parameters are in log scale
exp(coef(fit.pareto))            #estimate of parameters
confint(fit.pareto, level=0.95)  #confidence intervals for model parameters 
logLik(fit.pareto)               #loglikelihood for pareto
AIC(fit.pareto)                  #AIC for pareto
BIC(fit.pareto)                  #BIC for pareto
vcov(fit.pareto)                 #covariance matrix for model parameters 
```

#### User-Defined Likelihood Function
Here we estimate alpha and theta directly to define the pareto density.
```{r warning=FALSE , message=FALSE , comment=""}
dpareto <- function(y,theta,alpha){
  alpha*theta^alpha/(y+theta)^(alpha+1)
}
loglikP<-function(parms){ 
  theta=parms[1]
  alpha=parms[2]
  llk <- -sum(log(dpareto(sev.dat$Loss,theta,alpha)))
  llk
}
ini.P <- exp(coef(fit.pareto))
zop.P <- nlminb(ini.P,loglikP,lower=c(1e-6,1e-6),upper=c(Inf,Inf))
print(zop.P)
```

#### Obtain Standard Error
```{r warning=FALSE , message=FALSE , comment=""}
library(numDeriv)
est <- zop.P$par
names(est) <- c("theta","alpha")
hess<-hessian(loglikP,est)
se <-sqrt(diag(solve(hess)))
print(cbind(est,se))
```

### Histogram{.tabset}
prepare the display window parameters to properly fit the histograms
```{r warning=FALSE , message=FALSE , comment=""}
par(mfrow=c(1,2))
```

#### LN
```{r warning=FALSE , message=FALSE , comment=""}
hist(sev.dat$Loss,xlab="Total Losses",main="LN",breaks=100,freq=F,xlim=c(0,3e4),ylim=c(0,8e-4))
x <- seq(1,max(sev.dat$Loss),1)
mu <- zop.LN$par[1]
sigma <- zop.LN$par[2]
lines(x,dlnorm(x,mu,sigma),col="red")
```

#### Pareto
```{r warning=FALSE , message=FALSE , comment=""}
hist(sev.dat$Loss,xlab="Total Losses",main="Pareto",breaks=100,freq=F,xlim=c(0,3e4),ylim=c(0,8e-4))
x <- seq(1,max(sev.dat$Loss),1)
theta <- zop.P$par[1]
alpha <- zop.P$par[2]
lines(x,dpareto(x,theta,alpha),col="blue")
```

### qq Plots{.tabset}
#### Define Quantile Function of Pareto
```{r warning=FALSE , message=FALSE , comment=""}
qpareto <- function(p,theta,alpha){theta*((1-p)^(-1/alpha)-1)}
pct <- seq(0.01,0.99,0.01)
par(mfrow=c(1,2))
plot(qlnorm(pct,mu,sigma),quantile(sev.dat$Loss,probs=pct),
     main="LN", xlab="Theoretical Quantile", ylab="Empirical Quantile",
     xlim=c(0,7.5e4),ylim=c(0,7.5e4))
abline(0,1)
plot(qpareto(pct,theta,alpha),quantile(sev.dat$Loss,probs=pct),
     main="Pareto", xlab="Theoretical Quantile", ylab="Empirical Quantile",
     xlim=c(0,7.5e4),ylim=c(0,7.5e4))
abline(0,1)
```



